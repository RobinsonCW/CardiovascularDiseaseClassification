{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_GRID_SEARCH_CV = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../data/cardio_train.csv', delimiter=';')\n",
    "# set id as index\n",
    "df.set_index(\"id\", inplace=True)\n",
    "# copy original data\n",
    "df_clean = df.copy(deep=True)\n",
    "# drop duplicates\n",
    "df_clean.drop_duplicates(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# Convert age into years\n",
    "df_clean['age'] = (df_clean['age'] / 365).round().astype('int')\n",
    "\n",
    "# re-encode gender to 1 (male) and 0 (female)\n",
    "df_clean['gender'] = np.where((df_clean.gender == 2), 1, 0)\n",
    "\n",
    "# compute the body mass index based on weight and height\n",
    "df_clean['bmi'] = df_clean['weight'] / (df_clean['height']/100)**2\n",
    "\n",
    "# create a BMI group\n",
    "df_clean['bmiGrp'] = np.where((df_clean.bmi < 18.5), 1, 0)\n",
    "df_clean['bmiGrp'] = np.where((df_clean.bmi >= 18.5) & (df_clean.bmi < 25), 2, df_clean.bmiGrp)\n",
    "df_clean['bmiGrp'] = np.where((df_clean.bmi >= 25) & (df_clean.bmi < 30), 3, df_clean.bmiGrp)\n",
    "df_clean['bmiGrp'] = np.where((df_clean.bmi >= 30), 4, df_clean.bmiGrp)\n",
    "\n",
    "# bin blood pressure groups based on the api hi/ lo variables\n",
    "df_clean['bp'] = np.where((df_clean.ap_hi < 120) & (df_clean.ap_lo < 80), 1, 0)\n",
    "df_clean['bp'] = np.where((df_clean.ap_hi >= 120) & (df_clean.ap_hi < 130) & (df_clean.ap_lo < 80), 2, df_clean.bp)\n",
    "df_clean['bp'] = np.where((df_clean.ap_hi >= 130) & (df_clean.ap_hi < 140) | ((df_clean.ap_lo >= 80) & (df_clean.ap_lo < 90)), 3, df_clean.bp)\n",
    "df_clean['bp'] = np.where((df_clean.ap_hi >= 140) | (df_clean.ap_lo >= 90), 4, df_clean.bp)\n",
    "df_clean['bp'] = np.where((df_clean.ap_hi > 180) | (df_clean.ap_lo > 120), 5, df_clean.bp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Model\n",
    "# X_cols = ['age', 'gender', 'height', 'weight', 'ap_hi', 'ap_lo', 'cholesterol', 'gluc', 'smoke', 'alco', 'active']\n",
    "\n",
    "# New Feature Model\n",
    "X_cols = ['age', 'gender', 'bmiGrp', 'bp', 'cholesterol', 'gluc', 'smoke', 'alco', 'active']\n",
    "\n",
    "# Store feature matrix \n",
    "X = df_clean[X_cols] #.to_numpy()\n",
    "# Store response vector\n",
    "y = df_clean['cardio'] #.to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Models (50 points)\n",
    "\n",
    "Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use 80/20 training/testing split for your data). Adjust parameters of the models to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Estimators\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Model consisting of all original and new features with standardized values. RobustScaler below will scale features using statistics that are robutst to outliers.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=1)\n",
    "\n",
    "rs = RobustScaler()\n",
    "X_train_std = rs.fit_transform(X_train)\n",
    "X_test_std = rs.transform(X_test)\n",
    "\n",
    "# sc = StandardScaler()\n",
    "# X_train_std = sc.fit_transform(X_train)\n",
    "# X_test_std = sc.transform(X_test)\n",
    "\n",
    "# si = SimpleImputer(strategy=\"median\")\n",
    "# X_train_std = si.fit_transform(X_train_std)\n",
    "# X_test_std = si.transform(X_test_std)\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=-1, random_state=1)\n",
    "logreg.fit(X_train_std, y_train)\n",
    "\n",
    "fig = plt.figure(1, figsize=(20, 5))\n",
    "\n",
    "chart_1 = fig.add_subplot(121)\n",
    "chart_2 = fig.add_subplot(122)\n",
    "\n",
    "# Pass Fitted Model, and our test sets, see how they do\n",
    "plot_confusion_matrix(logreg, X_test_std, y_test, normalize='true', ax=chart_1)\n",
    "chart_1.set_title('Confusion Matrix')\n",
    "\n",
    "plot_roc_curve(logreg, X_test_std, y_test, ax=chart_2)\n",
    "chart_2.set_title('ROC Curve')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret Feature Importance (30)\n",
    "\n",
    "Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(logreg.coef_.T, X.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', round(coef[0], 3)) # now print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "coef_dict = {}\n",
    "\n",
    "for coef, feat in zip(logreg.coef_[0,:], X.columns):\n",
    "    coef_dict[feat] = coef\n",
    "\n",
    "coef_dict = OrderedDict({k: v for k, v in sorted(coef_dict.items(), key=lambda item: item[1])})\n",
    "  \n",
    "# weights = pd.Series(logreg.coef_[0],index=X.columns)\n",
    "weights = pd.Series(coef_dict).sort_values(ascending=False)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=1)\n",
    "\n",
    "# X_train = pd.get_dummies(X_train, columns=['gender', 'cholesterol', 'gluc', 'smoke', 'alco', 'active'])\n",
    "# X_test = pd.get_dummies(X_test, columns=['gender', 'cholesterol', 'gluc', 'smoke', 'alco', 'active'])\n",
    "\n",
    "# *dcrouthamel - Begin Section\n",
    "# New Feature Model\n",
    "X_cols = ['age', 'bmiGrp', 'bp', 'cholesterol']\n",
    "\n",
    "# Store feature matrix \n",
    "X = df_clean[X_cols] #.to_numpy()\n",
    "# Store response vector\n",
    "y = df_clean['cardio'] #.to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train = pd.get_dummies(X_train, columns=['cholesterol'])\n",
    "X_test = pd.get_dummies(X_test, columns=['cholesterol'])\n",
    "# *dcrouthamel - End Section\n",
    "\n",
    "rs = RobustScaler()\n",
    "X_train_std = rs.fit_transform(X_train)\n",
    "X_test_std = rs.transform(X_test)\n",
    "\n",
    "# use get dummies instead\n",
    "# ohe = OneHotEncoder()\n",
    "# X_train_std = ohe.fit_transform(X_train_std)\n",
    "# X_test_std = ohe.transform(X_test_std)\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=-1, random_state=1)\n",
    "logreg.fit(X_train_std, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(logreg.coef_.T, X_train.columns) # combine attributes\n",
    "zip_vars = zip_vars\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', round(coef[0], 3)) # now print them out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights = pd.Series(logreg.coef_[0], X_train.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(logreg, X_train_std, y_train,\n",
    "                         scoring=\"roc_auc\", cv=3)\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores.round(3))\n",
    "    print(\"Mean:\", scores.mean().round(3))\n",
    "    print(\"Standard deviation:\", scores.std().round(3))\n",
    "\n",
    "display_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if MODEL_TYPE == \"Full\":\n",
    "#     # Full Model\n",
    "#     num_attribs = [\"age\", \"height\", \"weight\", \"ap_hi\", \"ap_lo\"]\n",
    "#     cat_attribs = [\"gender\", \"cholesterol\", \"gluc\", \"smoke\", \"alco\", \"active\"]\n",
    "# else:\n",
    "# New Features\n",
    "num_attribs = [\"age\", \"bmiGrp\", \"bp\"]\n",
    "cat_attribs = [\"gender\", \"cholesterol\", \"gluc\", \"smoke\", \"alco\", \"active\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('rbs_scaler', RobustScaler()),\n",
    "        # ('std_scaler', StandardScaler()),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\"), cat_attribs),\n",
    "    ])\n",
    "\n",
    "X_prepared = full_pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Advantages (10)\n",
    "\n",
    "Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_prepared, y, stratify=y, test_size=0.2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(n_jobs=-1, C=2, random_state=1)\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "fig = plt.figure(1, figsize=(20, 5))\n",
    "\n",
    "chart_1 = fig.add_subplot(121)\n",
    "chart_2 = fig.add_subplot(122)\n",
    "\n",
    "plot_confusion_matrix(logreg, X_test, y_test, normalize='true', ax=chart_1)\n",
    "chart_1.set_title('Confusion Matrix')\n",
    "\n",
    "plot_roc_curve(logreg, X_test, y_test, ax=chart_2)\n",
    "chart_2.set_title('ROC Curve')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.svm import LinearSVC\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "model_params = {\n",
    "    \"sgd\": {\n",
    "        \"model\": SGDClassifier(),\n",
    "        \"params\": {\n",
    "            \"alpha\": [0.00001, 0.0001, 0.001, 0.01, 0.1],\n",
    "            \"class_weight\": [\"balanced\", None]\n",
    "        }\n",
    "    },\n",
    "    \"logistic_regression\": {\n",
    "        \"model\": LogisticRegression(),\n",
    "        \"params\": {\n",
    "            \"C\": [1, 2, 5, 10, 15, 20]\n",
    "        }\n",
    "    },\n",
    "    \"linear_svc\": {\n",
    "        \"model\": LinearSVC(),\n",
    "        \"params\": {\n",
    "            \"C\": [1, 2, 5, 10, 15, 20], \n",
    "            \"class_weight\": [\"balanced\", None]\n",
    "        }\n",
    "    },\n",
    "    # \"svc\": {\n",
    "    #     \"model\": SVC(),\n",
    "    #     \"params\": {\n",
    "    #         \"C\": [1, 10, 100, 1000],\n",
    "    #         \"kernel\": [\"rbf\"],\n",
    "    #         \"gamma\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    #     }\n",
    "    # },\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (RUN_GRID_SEARCH_CV):\n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for model_name, mp in model_params.items():\n",
    "        clf = GridSearchCV(estimator = mp[\"model\"], param_grid=mp[\"params\"], cv=10, scoring=\"roc_auc\", n_jobs=-1)\n",
    "        clf.fit(X_train, y_train)\n",
    "        scores.append({\"model\": model_name,\n",
    "        \"best_score\": clf.best_score_, # Mean cross-validated score of the best_estimator\n",
    "        \"best_params\": clf.best_params_\n",
    "        })\n",
    "\n",
    "    df_grid_search_scores = pd.DataFrame(scores, columns=[\"model\", \"best_score\", \"best_params\"])\n",
    "    print(df_grid_search_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svm_sgd = SGDClassifier(alpha=0.001, class_weight=\"balanced\", n_jobs=-1, random_state=1) # get object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_sgd.fit(X_train, y_train)  # train object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_sgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svm_lin = LinearSVC(C=15, class_weight=None) # get object\n",
    "svm_lin.fit(X_train, y_train)  # train object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_lin.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svm_best = SVC(kernel='rbf', C=1, gamma=0.1, class_weight=\"balanced\", random_state=1) # get object\n",
    "svm_best = SVC() # get object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_best.fit(X_train, y_train)  # train object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(20, 5))\n",
    "\n",
    "chart_1 = fig.add_subplot(121)\n",
    "chart_2 = fig.add_subplot(122)\n",
    "\n",
    "plot_confusion_matrix(svm_best, X_test, y_test, normalize='true', ax=chart_1)\n",
    "chart_1.set_title('Confusion Matrix')\n",
    "\n",
    "plot_roc_curve(svm_best, X_test, y_test, ax=chart_2)\n",
    "chart_2.set_title('ROC Curve')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret Support Vectors (10)\n",
    "\n",
    "Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC model— then analyze the support vectors from the subsampled dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('ML7331': conda)",
   "language": "python",
   "name": "python37164bitml7331condaae43abe690ed4dc19794210c0252c813"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}