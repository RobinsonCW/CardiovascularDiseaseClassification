{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Part 1 (10 points)\n",
    "\n",
    "\n",
    "Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Imports\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Pre-Processing\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Metrics and Evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "# Train/ Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Imbalanced Data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "# Estimators\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Hyper Parameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# T-Tests\n",
    "from mlxtend.evaluate import paired_ttest_5x2cv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../../data/cardio_train.csv', delimiter=';')\n",
    "# set id as index\n",
    "df.set_index(\"id\", inplace=True)\n",
    "# copy original data\n",
    "df_clean = df.copy(deep=True)\n",
    "# drop duplicates\n",
    "df_clean.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# Convert age into years\n",
    "df_clean['age'] = (df_clean['age'] / 365).round().astype('int')\n",
    "\n",
    "# re-encode gender to male (1) and female (0)\n",
    "df_clean['gender'] = np.where((df_clean.gender == 2), 1, 0)\n",
    "\n",
    "# compute the body mass index based on weight and height\n",
    "df_clean['bmi'] = df_clean['weight'] / (df_clean['height']/100)**2\n",
    "\n",
    "# create a BMI group\n",
    "df_clean['bmiGrp'] = np.where((df_clean.bmi < 18.5), 1, 0)\n",
    "df_clean['bmiGrp'] = np.where((df_clean.bmi >= 18.5) & (df_clean.bmi < 25), 2, df_clean.bmiGrp)\n",
    "df_clean['bmiGrp'] = np.where((df_clean.bmi >= 25) & (df_clean.bmi < 30), 3, df_clean.bmiGrp)\n",
    "df_clean['bmiGrp'] = np.where((df_clean.bmi >= 30), 4, df_clean.bmiGrp)\n",
    "\n",
    "# bin blood pressure groups based on the api hi/ lo variables\n",
    "df_clean['bp'] = np.where((df_clean.ap_hi < 120) & (df_clean.ap_lo < 80), 1, 0)\n",
    "df_clean['bp'] = np.where((df_clean.ap_hi >= 120) & (df_clean.ap_hi < 130) & (df_clean.ap_lo < 80), 2, df_clean.bp)\n",
    "df_clean['bp'] = np.where((df_clean.ap_hi >= 130) & (df_clean.ap_hi < 140) | ((df_clean.ap_lo >= 80) & (df_clean.ap_lo < 90)), 3, df_clean.bp)\n",
    "df_clean['bp'] = np.where((df_clean.ap_hi >= 140) | (df_clean.ap_lo >= 90), 4, df_clean.bp)\n",
    "df_clean['bp'] = np.where((df_clean.ap_hi > 180) | (df_clean.ap_lo > 120), 5, df_clean.bp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses two datasets, a cardiovascular dataset and a smoking dataset.  Each dataset is derived from the same data.  This data was uploaded to the Kaggle website by Svetlana Ulianova, who appears to be a data science student at Ryerson University.  Ms. Ulianova did not provide a source, or exact purpose of the dataset.  Further, Ms. Ulianova did not sponsor a competition or add a description with the dataset.  The data provided on Kaggle includes 70,000 rows and 12 different features, including whether a patient has cardiovascular disease and if a patient is a smoker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response variable in the cardiovascular dataset is whether a person has cardiovascular disease, which is denoted in the dataset as the attribute “cardio.”  The response variable in the smoking dataset is whether a person is a smoker, which is denoted in the dataset as the attribute “smoke.”  The analysis provides a prediction of whether a person has the presence or absence of cardiovascular disease, which is indicated by a column in the cardiovascular disease dataset titled “cardio,” and whether a person is or is not smoker, which is indicated by a column in the smoking dataset titled “smoke.”  The “cardio” and “smoke” columns in each of the cardiovascular disease and smoking datasets are binary, categorical variables. Therefore, the modeling problem for each of the cardiovascular disease and smoking datasets is defined as a classification problem. An effective outcome may be measured using the receiver operating characteristic and recall metrics, discussed below in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook develops models to predict whether a person has cardiovascular disease and whether a person is a smoker based on the available data.  Therefore, the class variables are “Cardiovascular Disease” and “Smoking.”  Both class variables “Cardiovascular Disease” and “Smoking” are binary, categorical variables.  Further variable representations can be seen below in Table 1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table 1: Cardiovascular Dataset - Attribute Descriptions**\n",
    "\n",
    "| Column Description | Feature Type | Column Name | Data Type |\n",
    "|:---|:---|:---|:---|\n",
    "| **Age**                        | Objective | age | int (days) |\n",
    "| **Height**                     | Objective | height | int (cm) |\n",
    "| **Weight**                     | Objective | weight | float (kg) |\n",
    "| **Gender**                     | Objective | gender | 0: female, 1: male |\n",
    "| **Systolic blood pressure**    | Examination | ap_hi | int |\n",
    "| **Diastolic blood pressure**   | Examination | ap_lo | int |\n",
    "| **Cholesterol**                | Examination | cholesterol | 1: normal, 2: above normal, 3: well above normal |\n",
    "| **Glucose**                    | Examination | gluc | 1: normal, 2: above normal, 3: well above normal |\n",
    "| **Smoking**                    | Subjective | smoke | binary |\n",
    "| **Alcohol intake**             | Subjective | alco | binary |\n",
    "| **Physical activity**          | Subjective | active | binary |\n",
    "| **Has CVD?**                   | Target | cardio | binary |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in Table 1, the datasets used for classifying whether a person has cardiovascular disease, and if a person is a smoker use 11 features and 1 target variable, being having cardiovascular disease for the cardiovascular disease dataset and is a smoker for the smoking dataset, respectively. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 11 features represent three different types of input features.  These include objective features having factual information, examination features having medical examination information, and subjective features having patient response information.  Further, the datasets each include 70,000 rows used in each of the cardiovascular disease dataset and the smoking dataset.  To examine and provide classification of the presence of cardiovascular disease and whether a person is a smoker, relationships between each of the features using all of the rows by imputing, scaling, or one-hot encoding, values using a median or mean value for those that are missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain pre-processing techniques are used to prepare both the cardiovascular disease and smoking datasets.  These pre-processing methods use a variety of libraries used for data analysis in Python.  The libraries used for pre-processing include the “RobustScaler,” “StandardScaler,” “OneHotEncoder,” “SimpleImputer,” and “ColumnTransformer” functions within the scikit-learn package.  Specifically, the “RobustScaler,” “StandardScaler,” and “OneHotEncoder” functions are used in the “preprocessing” component of the scikit-learn package while the “SimpleImputer,” and “ColumnTransformer” are used in the “impute” and “compose” components of the scikit-learn package, respectively.  The RobustScaler() function from the scikit-learn python package is used to standardize values between features.  The RobustScaler() function scales features using statistical analysis that is robust to outliers in a dataset.  (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html).  A variety of different parameters may be adjusted to improve performance of each of the models.  For example, instead of using the RobustScaler() function, the StandardScalar() function may be implemented from the scikit-learn python package.  The StandardScalar() function removes the mean and scales to a unit variance to standardize the data. \n",
    "(https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).  \n",
    "While not explicitly included in the preprocessing cell, these functions are implemented in a pipeline in later cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A one-hot encoding (OHE) has been implemented to standardize attributes such as gender and cholesterol.  However, since these categorical attributes were recorded as numerical variables when recorded into the dataset, OHE is not expected to impact the performance of the classifiers.  For example, gender was recorded as between 1 or 2 instead of 0 and 1, which is still indicative of an ordinal relationship.  Therefore, OHE may aid in standardizing the data, but in this instance, not impact performance of the models because of the encoding that occurred when the data were recorded.  Stated sufficiently, OHE may be redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, the data is loaded into a Python dataframe “df,” which is then copied into another Python dataframe “df_clean.”  Duplicates are dropped in the “df_clean” dataframe.  Next, the variable “age,” which is an integer in days, is converted into years while maintaining the type “int” for the “age” variable.  The categorical variable “gender” was originally encoded as being between “1” and “2.”  For consistency and clarity of interpretation, the “gender” variable was reencoded to define the input “male” as “1” and “female” as “0” as described.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering is also performed using the “height” and “weight” variables.  The variable “height” is an integer while the variable “weight” is a float.  Generally, the height and weight of an individual are related through the “Body Mass Index” (BMI) such that the weight divided by the squared quantity of height divided by 100 provides an indication of the BMI of a person.  Both the cardiovascular disease and smoking datasets use the “height” and “weight” variables provided to calculate and use BMI as a variable in the cardiovascular disease and smoking datasets.  Further, after the BMI is calculated for each dataset, the BMI is grouped based on resulting numerical, BMI values.  The BMI values are separated into 4 categories based on the BMI value calculated.  A new variable “bmiGrp” is used to separate the 4 new categories of BMI.  Specifically, a BMI value being less than 18.5 is represented as “1” in the “bmiGrp” variable, a BMI value being between 18.5 and 25 is represented as “2” in the “bmiGrp” variable, a BMI value between 25 and 30 is represented as “3” in the “bmiGrp” variable, and a BMI value being greater than 30 is represented as “4” in the “bmiGrp” variable.  Binning the BMI variable may provide greater indication of whether a person has cardiovascular disease or is a smoker by allowing for an analysis of potential separation of the groups.  For example, through comparing those who have cardiovascular disease and are smokers across groups, prediction accuracy my improve based on separation between groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the variables “ap_hi” and “ap_lo” represent systolic and diastolic blood pressure, respectively.  Both variables are represented as integers.  The variables are combined and binned into a single variable representing blood pressure.  Specifically, the “ap_hi” and “ap-lo” variables are combined into a new variable labeled “bp” and binned into 5 distinct categories.  Specifically, the “bp” variable includes a category represented by “1” that divides blood pressure such that the “ap_hi” variable is less than 120 and the “ap_lo” variable is less than 80, a category represented by “2” that divides blood pressure such that the “ap_hi” variable is between 120 and 130 and the “ap_lo” variable is less than 80, a category represented by “3” that divides blood pressure such that the “ap_hi” variable is between 130 and 140 and the “ap_lo” variable is between 80 and 90,\n",
    "a category represented by “4” that divides blood pressure such that the “ap_hi” variable is greater than or equal to 140 or the “ap_lo” variable greater than or equal to 90, and a category represented by “5” that divides blood pressure such that the “ap_hi” variable is greater than 180 or the “ap_lo” variable is greater than 120.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Part 2 (5 points)\n",
    "\n",
    "\n",
    "Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two, final datasets, a cardiovascular dataset and a smoking dataset.  The response variable in the cardiovascular dataset is whether a person has cardiovascular disease, which is denoted in the dataset as the attribute “cardio.”  The response variable in the smoking dataset is whether a person is a smoker, which is denoted in the dataset as the attribute “smoke.”  The analysis provides a prediction of whether a person has the presence or absence of cardiovascular disease, which is indicated by a column in the cardiovascular disease dataset titled “cardio,” and whether a person is or is not smoker, which is indicated by a column in the smoking dataset titled “smoke.”  The “cardio” and “smoke” columns in each of the cardiovascular disease and smoking datasets are binary, categorical variables. Therefore, the modeling problem for each of the cardiovascular disease and smoking datasets is defined as a classification problem. An effective outcome may be measured using the receiver operating characteristic and recall metrics, discussed below in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the variables created and categorized, the final dataset used for classifying the presence of cardiovascular disease includes columns “age,” “gender,” “bmiGrp,” “bp,” “cholesterol,” “gluc,” “smoke,” “alco,” and “active.”  Similarly, the final dataset used for classifying whether a person is a smoker includes columns “age,” “gender,” “bmiGrp,” “bp,” “cholesterol,” “gluc,” “cardio,” “alco,” and “active.”  The cardiovascular disease dataset has 9 attributes that are used to classify the response variable of “cardio” indicative of having cardiovascular disease.  The smoking dataset also has 9 attributes that are used to classify the response variable of “smoke” indicative of whether a person is a smoker.  Both the cardiovascular disease and smoking datasets use all the rows from the original dataset, or 70,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed above, variables “bp” and “bmiGrp” are newly formed variables used in the analysis of the cardiovascular disease and smoking datasets.  The “bp” variable combines the “ap_hi” and “ap_lo” variables into a single category based on individual values in each of those attributes.  This allows for an analysis of whether a combined blood pressure may be indicative of cardiovascular disease by examining and comparing the categories.  A similar analysis is used to analyze whether a combined blood pressure may be indicative of smoking.  The newly formed “bmiGrp” is based on a calculated BMI value from the listed height and weight of each row that was recorded for each of the cardiovascular disease and smoking datasets.  Again, the “bmiGrp” bins rows into categories based on the calculated values.  This also allows for an analysis of whether a BMI above a threshold is indicative of cardiovascular disease.  Similarly, the “bmiGrp” variable provides an analysis of whether BMI being above a threshold is indicative of a person being a smoker.  Each of the “bmiGrp” and “bp” variables use all of the rows in each of the cardiovascular disease and smoking datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Store feature matrix for different tasks (has CVD) and (is smoker)\n",
    "X_cols_cvd = ['age', 'gender', 'bmiGrp', 'bp', 'cholesterol', 'gluc', 'smoke', 'alco', 'active']\n",
    "X_cols_smk = ['age', 'gender', 'bmiGrp', 'bp', 'cholesterol', 'gluc', 'cardio', 'alco', 'active']\n",
    "\n",
    "X_cvd = df_clean[X_cols_cvd]\n",
    "X_smk = df_clean[X_cols_smk]\n",
    "\n",
    "y_cardio = df_clean['cardio']\n",
    "y_smoke = df_clean['smoke']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 69976 entries, 0 to 99999\nData columns (total 15 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   age          69976 non-null  int32  \n 1   gender       69976 non-null  int32  \n 2   height       69976 non-null  int64  \n 3   weight       69976 non-null  float64\n 4   ap_hi        69976 non-null  int64  \n 5   ap_lo        69976 non-null  int64  \n 6   cholesterol  69976 non-null  int64  \n 7   gluc         69976 non-null  int64  \n 8   smoke        69976 non-null  int64  \n 9   alco         69976 non-null  int64  \n 10  active       69976 non-null  int64  \n 11  cardio       69976 non-null  int64  \n 12  bmi          69976 non-null  float64\n 13  bmiGrp       69976 non-null  int32  \n 14  bp           69976 non-null  int32  \ndtypes: float64(2), int32(4), int64(9)\nmemory usage: 7.5 MB\n"
    }
   ],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 1 (10 points)\n",
    "\n",
    "\n",
    "Choose and explain your evaluation metrics that you will use (i.e., accuracy,\n",
    "precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response variable in the cardiovascular dataset is whether a person has cardiovascular disease, which is denoted in the dataset as the attribute “cardio.”  The analysis provides a prediction of whether a person has the presence or absence of cardiovascular disease, which is indicated by a column in the cardiovascular disease dataset titled “cardio.”  The “cardio” column of the cardiovascular disease dataset is a binary, categorical variable. Therefore, the modeling problem for the cardiovascular disease dataset is defined as a classification problem. Since the response variable, cardiovascular disease, is balanced, an effective outcome may be measured using the receiver operating characteristic metric, discussed below in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cardiovascular disease dataset uses the receiver operating characteristic (“ROC”) metric to evaluate performance of the classification models.  The ROC metric characterizes a trade-off between true and false positives in a classification model.  Here, the ROC metric is used as a threshold related to the accuracy of the classification model used for cardiovascular disease.  The ROC graph compares a curve of false positives by true positives against a straight, diagonal line indicative of random guessing.  The area under the curve compared against the diagonal line is indicative of performance of the classification model.  For example, a model having greater area under the curve closer to the origin provides better classification accuracy for small false-positive rates, whereas a model having greater area under the curve further away from the origin, closer to 1 provides better classification accuracy for larger false-positive rates.  The false-positive rate is equal to the false-positive values divided by the total number of negative values.  Likewise, the true-positive rate is equal to the true-positive values divided by the total number of positive values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC metric is appropriate to analyze the classification models on the cardiovascular disease dataset because the cardiovascular disease dataset is a balanced response variable, in this case cardiovascular disease.  Stated differently, the response variable, being cardiovascular disease has a high number of values being indicative of a lot of people having cardiovascular disease, and, therefore, the ROC metric provides a robust metric for evaluating performance of a model using the false-positive rates of each of the models.  The ROC metric provides an easily interpretable and visual metric to evaluate the performance of the classification models.  Further, in the instance of predicting the presence of cardiovascular disease, the false-positive rate is an important metric to evaluate.  For example, treating cardiovascular disease when no cardiovascular disease is present, a false-positive, is harmful to an otherwise healthy patient.  Stated differently, a patient that undergoes a surgical intervention without having cardiovascular disease is subjected to significant, unnecessary trauma.  In the absence of a surgical intervention, a healthy patient taking medication unnecessarily still provides harm to the patient, such as through side effects and secondary reactions to medications.  Therefore, when evaluating models designed to predict the presence of cardiovascular disease, the ROC metric and curve provides a method to analyze which models have the lowest false-positive rates when predicting the presence of cardiovascular disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response variable in the smoking dataset is whether a person is a smoker, which is denoted in the dataset as the attribute “smoke.” The analysis provides a prediction of whether a person is or is not smoker, which is indicated by a column in the smoking dataset titled “smoke.”  The “smoke” column of the smoking dataset is a binary, categorical variable. Therefore, the modeling problem for the smoking dataset is defined as a classification problem. Since the smoking dataset is not balanced, an effective outcome may be measured using the recall metric, discussed below in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smoking dataset uses the recall metric to evaluate performance of the classification models.  Again, the response variable, being whether a person smokes, is imbalanced, which implies that there are very few smokers included in the dataset.  This low amount of values representing smokers makes the ROC curve and metric unsuitable for model performance evaluation.  The recall metric provides an indication of the false negatives in the classification performance of the model.  Recall is a cost-sensitive measure used to analyze performance of a classification model based on the false negatives predicted from the model.  Specifically, recall is equal to the total true-positive predictions divided by the total quantity of the true-positive predictions added to the total false-negative values from the predictions of the classification models.  When evaluating the recall metric, the higher the recall value, less false-negative values are present in the prediction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall metric is appropriate to analyze the classification models on the smoking dataset because the smoking dataset has an imbalanced response variable, in this case smoking.  Stated differently, the response variable, being smoking has a low number of values being indicative of very few people being smokers, and, therefore, the recall metric provides a robust metric for evaluating performance of a model using the false-negative rates of each of the models.  The recall metric provides an easily interpretable metric to evaluate the performance of the classification models such that the higher recall value, the lower number of false negatives are being predicted.  Further, in the instance of predicting whether a person is a smoker, the false-negative rate is an important metric to evaluate.  For example, when determining whether a person is a smoker, such as during a health insurance premium determination, reducing the false-negative values, that is smokers predicted as not smokers is valuable.  Stated differently, a person who smokes presents a higher risk for needing long-term hospital care based on the damage to the lungs from the smoking.  Being at higher risk for expensive, future medical bills, a health insurance company may charge higher premiums to offset this voluntary risk.  Predicting someone as not a smoker when they do smoke, a false-negative, increases the risk that the health insurance company will need to pay for expensive medical bills without having recovered some of the losses with higher premiums.  Therefore, when evaluating models designed to predict whether a person is a smoker, the recall metric provides a method to analyze which models have the lowest false-negative rates when predicting if a person is a smoker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To account for the imbalance in the smoking dataset, the “imbalanced-learn” Python package may be used to up sample the smoking dataset.  Specifically, using the “Pipeline” and “SMOTE” functions from the “pipeline” and “over_sampling” libraries, respectively.  This up sampling of the smoking dataset response variable allows the smoking dataset to also be evaluated using the ROC metric, as above described.  The ROC metric in the smoking dataset measures the false-positive rate of the response variable, or in this instance, a measure of the predictions of those who do not smoke as those who do smoke.  In the example provided, this would cause a person who is not a smoker to incur costs of someone who does smoke, which causes unnecessary expenses on those people labeled smokers who are not smokers.  Therefore, in order to reduce this type of error, the ROC metric is appropriate to evaluate models run on the smoking dataset, after the smoking dataset is up sampled, as described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 2 (10 points)\n",
    "\n",
    "Choose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response variable in the cardiovascular dataset is whether a person has cardiovascular disease, which is denoted in the dataset as the attribute “cardio.”  The analysis provides a prediction of whether a person has the presence or absence of cardiovascular disease, which is indicated by a column in the cardiovascular disease dataset titled “cardio.”  The “cardio” column of the cardiovascular disease dataset is a binary, categorical variable. Therefore, the modeling problem for the cardiovascular disease dataset is defined as a classification problem. Since the response variable, cardiovascular disease, is balanced, stratified cross-validation is used to divide the cardiovascular disease dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method used to divide the data into training and testing splits is stratified cross-validation.  Specifically, the method used is a 10-fold, stratified cross-validation with an 80/20 training/testing split.  Generally, cross-validation partitions data into a number of disjoint subsets.  The number of folds, in this instance 10, represents the number of disjoint subsets.  For example, a 10-fold cross validation trains on (10-1), or 9, partitions with testing being done the remaining partition.  Training the classifier using 9 partitions allows for separation between the testing and training data such that the classifier does not train on the testing data.  Keeping the testing data separate from the training data ensures that the metrics used to evaluate each of the classifiers for the cardiovascular disease dataset provide an accurate indication of performance of the model.  This is why the cardiovascular disease dataset uses an 80/20 split, being indicative of 80% of the data being used for training and 20% of the data being separated for testing.  As stated, however, the cardiovascular disease dataset uses stratified cross validation to split the testing and training data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified cross validation uses two different classes having the same frequency in each fold of the cross validation.  This typically occurs after 10 folds in stratified cross validation, and thus the number of folds used with the cardiovascular disease dataset is set to 10.  Therefore, the method used to divide the cardiovascular disease dataset is a 10-fold, stratified cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 10-fold, stratified cross validation is appropriate for the cardiovascular disease dataset because the 10-fold, stratified cross validation ensures an equal amount of positive and negative classes that occur with the same frequency, as described.  Since, as discussed above, the important metric for analysis with the cardiovascular disease dataset is the false-positive rate, having an equal distribution of positive and negative classes ensures that the false-positive rate, as evaluated by the ROC metric and curve, is not skewed.  For example, with very people having cardiovascular disease, or with very many people with cardiovascular disease, a number of false positives may appear to have a higher or lower rate due to the imbalance in the data.  Since the 10-fold, stratified cross validation ensures the same frequency between positive and negative classes, the false-positive rate is normalized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response variable in the smoking dataset is whether a person is a smoker, which is denoted in the dataset as the attribute “smoke.” The analysis provides a prediction of whether a person is or is not smoker, which is indicated by a column in the smoking dataset titled “smoke.”  The “smoke” column of the smoking dataset is a binary, categorical variable. Therefore, the modeling problem for the smoking dataset is defined as a classification problem. Since the smoking dataset is not balanced, an effective method for dividing the smoking dataset is again the 10-fold, stratified cross validation using an 80/20 training/testing split of the smoking dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in explicit detail above, the 10-fold stratified cross validation method provides a balance between positive and negative classes.  Here, the response variable of whether a person is a smoker, has an imbalance in the data.  Stated differently, the smoking dataset includes very few smokers.  This skews the data and as such, the recall metric being used to evaluate the models since a few false-negative values that occur with an imbalanced dataset skews the rate of occurrence of the false-negative values found the model.  Therefore, to account for this imbalance on the response variable in the smoking dataset, the 10-fold, stratified cross validation is used to eliminate or reduce skew caused by an unequal amount of positive and negative classes.  The imbalance in the response variable of the smoking dataset, being whether a person is a smoker, makes using the 10-fold, stratified cross validation the most appropriate method for dividing the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, as described above, the smoking dataset may be up sampled to account for the imbalance of the categorical response variable of whether a person is a smoker.  Again, the Python package “imbalanced-learn” can be used to up-sample the smoking dataset to correct the imbalance in the smoking dataset response variable.  Using the “imbalanced-learn” Python package makes the 10-fold, stratified cross validation less important based on the dataset achieving balance with the up sample.  However, the 10-fold, stratified cross validation is still the most appropriate method to use to divide the smoking dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, both the cardiovascular disease and smoking datasets use a 10-fold, stratified cross validation method to separate the data into training and testing partitions.  The 10-fold stratified cross validation is the most appropriate method to divide data from both the cardiovascular disease and smoking datasets into training and testing splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10-fold cross validation:\n\nROC AUC: 0.703 (+/- 0.01) [Naive Bayes]\nROC AUC: 0.777 (+/- 0.01) [KNN]\nROC AUC: 0.768 (+/- 0.01) [Logistic Regression]\n"
    }
   ],
   "source": [
    "# Going to use all of data\n",
    "X = X_cvd\n",
    "y = y_cardio\n",
    "\n",
    "numeric_features = ['age', 'cholesterol', 'bp', 'bmiGrp', 'gluc']\n",
    "categorical_features = ['gender', 'smoke', 'alco', 'active']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"median\"))])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"median\")),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "        \n",
    "\n",
    "clf1 = MultinomialNB(alpha=1.0)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=49)\n",
    "clf5 = LogisticRegression(random_state=1, penalty='l2', C=.01)\n",
    "\n",
    "\n",
    "pipe1 = Pipeline([['preprocessor', preprocessor], \n",
    "                 # ['rs', RobustScaler()],             <<< ValueError: Negative values in data passed to MultinomialNB (input X)\n",
    "                 ['clf', clf1]])\n",
    "\n",
    "pipe2 = Pipeline([['preprocessor', preprocessor], \n",
    "                  ['rs', RobustScaler()],\n",
    "                  ['clf', clf2]])\n",
    "\n",
    "pipe5 = Pipeline([['preprocessor', preprocessor], \n",
    "                  ['rs', RobustScaler()],\n",
    "                  ['clf', clf5]])\n",
    "\n",
    "# clf_labels = ['Naive Bayes', 'KNN', 'Random Forest', 'Gradient Boosting', 'Logistic Regression', 'Decision Tree']\n",
    "clf_labels = ['Naive Bayes', 'KNN', 'Logistic Regression']\n",
    "\n",
    "# Note n_jobs below. Setting it to -1 will create cv number of threads\n",
    "print('10-fold cross validation:\\n')\n",
    "for clf, label in zip([pipe1, pipe2, pipe5], clf_labels):\n",
    "    scores = cross_val_score(estimator=clf,\n",
    "                             X=X,\n",
    "                             y=y,\n",
    "                             cv=10,\n",
    "                             scoring='roc_auc',\n",
    "                             n_jobs=-1)\n",
    "    print(\"ROC AUC: %0.3f (+/- %0.2f) [%s]\"\n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same as above, but against smoke set and using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10-fold cross validation:\n\nRecall: 0.675 (+/- 0.01) [Random Forest]\nRecall: 0.868 (+/- 0.01) [Gradient Boosting]\nRecall: 0.840 (+/- 0.03) [Decision Tree]\n"
    }
   ],
   "source": [
    "# Going to use all of data\n",
    "X = X_smk\n",
    "y = y_smoke\n",
    "\n",
    "smt = SMOTE(sampling_strategy='not majority')\n",
    "# smt = BorderlineSMOTE(sampling_strategy='not majority')\n",
    "\n",
    "numeric_features = ['age', 'cholesterol', 'bp', 'bmiGrp', 'gluc']\n",
    "categorical_features = ['gender', 'cardio', 'alco', 'active']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"median\"))])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"median\")),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "\n",
    "clf3 = RandomForestClassifier(random_state=1, n_estimators=200, min_samples_split=10, min_samples_leaf=4, max_features='sqrt', bootstrap=True)\n",
    "clf4 = GradientBoostingClassifier(random_state=1, n_estimators=8, min_samples_split=0.30000000000000004, min_samples_leaf=0.2, max_features=2, max_depth=15.0, loss='exponential')\n",
    "clf6 = DecisionTreeClassifier(random_state=1, splitter='random', min_samples_split=5, min_samples_leaf=4, max_features='auto', criterion='entropy', class_weight=None)\n",
    "\n",
    "\n",
    "\n",
    "pipe3 = Pipeline([['preprocessor', preprocessor], \n",
    "                  ['rs', RobustScaler()],\n",
    "                  ['smt', smt],\n",
    "                  ['clf', clf3]])\n",
    "                  \n",
    "pipe4 = Pipeline([['preprocessor', preprocessor], \n",
    "                  ['rs', RobustScaler()],\n",
    "                  ['smt', smt],\n",
    "                  ['clf', clf4]])\n",
    "\n",
    "pipe6 = Pipeline([['preprocessor', preprocessor], \n",
    "                  ['rs', RobustScaler()],\n",
    "                  ['smt', smt],\n",
    "                  ['clf', clf6]])\n",
    "\n",
    "# clf_labels = ['Naive Bayes', 'KNN', 'Random Forest', 'Gradient Boosting', 'Logistic Regression', 'Decision Tree']\n",
    "clf_labels = ['Random Forest', 'Gradient Boosting', 'Decision Tree']\n",
    "\n",
    "# Note n_jobs below. Setting it to -1 will create cv number of threads\n",
    "print('10-fold cross validation:\\n')\n",
    "for clf, label in zip([pipe3, pipe4, pipe6], clf_labels):\n",
    "    scores = cross_val_score(estimator=clf,\n",
    "                             X=X,\n",
    "                             y=y,\n",
    "                             cv=10,\n",
    "                             scoring='recall',\n",
    "                             n_jobs=-1)\n",
    "    print(\"Recall: %0.3f (+/- %0.2f) [%s]\"\n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check prediction against entire set using SMOTE, and just the Tree classifier (we could have used any, this is just an example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "accuracy: 0.9376500514462102\nf1: 0.4929692039511911\nrecall: 0.34381585346085264\nprecision: 0.8706896551724138\n"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(X,y)\n",
    "yhat = dt_clf.predict(X)\n",
    "print ('accuracy:', metrics.accuracy_score(y,yhat))\n",
    "\n",
    "print('f1:', metrics.f1_score(y,yhat))\n",
    "\n",
    "print('recall:', metrics.recall_score(y,yhat))\n",
    "\n",
    "print('precision:', metrics.precision_score(y,yhat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now do same test, but against the imbalanced set (No SMOTE). The accuracy of this set is higher, but has a much lower recall score compared to SMOTE above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "accuracy: 0.9376500514462102\nf1: 0.4929692039511911\nrecall: 0.34381585346085264\nprecision: 0.8706896551724138\n"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(X,y)\n",
    "yhat = dt_clf.predict(X)\n",
    "print ('accuracy:', metrics.accuracy_score(y,yhat))\n",
    "\n",
    "print('f1:', metrics.f1_score(y,yhat))\n",
    "\n",
    "print('recall:', metrics.recall_score(y,yhat))\n",
    "\n",
    "print('precision:', metrics.precision_score(y,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 3\n",
    "\n",
    "Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the cardiovascular disease dataset and the smoking dataset involve classification problems.  The response variable in the cardiovascular dataset is whether a person has cardiovascular disease, which is denoted in the dataset as the attribute “cardio.”  The response variable in the smoking dataset is whether a person is a smoker, which is denoted in the dataset as the attribute “smoke.”  The analysis provides a prediction of whether a person has the presence or absence of cardiovascular disease, which is indicated by a column in the cardiovascular disease dataset titled “cardio,” and whether a person is or is not smoker, which is indicated by a column in the smoking dataset titled “smoke.”  The “cardio” and “smoke” columns in each of the cardiovascular disease and smoking datasets are binary, categorical variables.  Therefore, 3 classification models for each of the cardiovascular disease dataset and the smoking dataset are analyzed and compared.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cardiovascular disease dataset, the classification models include a Naïve-Bayes, k-NN, and logistic regression algorithm.  Initially, using all of the data, the attributes are split into numerical and categorical features.  This allows for the pre-processing discussed above, such as imputing a median value, to be done through pipelines, which provide an efficient technique for processing data.  Using these pipelines, the cardiovascular disease dataset is passed through to the classification algorithms, in this case Naïve-Bayes, k-NN, and logistic regression algorithms.  Again, performance of these algorithms is evaluated against the ROC metric to understand the false-positive rate of each of the algorithms.  Again, the closer the value to 1, the ROC metric and curve measure more area under the curve.  Applying this metric to each of the classification models determines which of the models have the greatest accuracy when predicting the presence of cardiovascular disease.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the 10-fold, stratified cross validation, each of the classification models calculated a ROC value.  Shown in the below cell, the ROC for the Naïve-Bayes model is `0.703`, the ROC for the k-NN model is `0.773`, and the ROC for the logistic regression model is `0.768`.  Therefore, in comparison, the k-NN model provides the greatest ROC value between the competing models and provides the greatest indication of predicting cardiovascular disease.  This suggests that the most appropriate model to predict cardiovascular disease can have arbitrary decision boundaries, in which uses a distance metric to compute distance between records.  The next closest performing model is the logistic regression model.  The ROC from the logistic regression only slightly less than the ROC for the k-NN model, while the Naïve-Bayes model is much less than both the k-NN and logistic regression models.  This suggests that, while the ROC for the k-NN and logistic regression are competitive, the Naïve-Bayes model is not the most appropriate model to predict the presence of cardiovascular disease in a patient.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, while the k-NN and logistic regression models have ROC values greater than `0.75`, in the domain of disease, and specifically cardiovascular disease, a much higher ROC value is needed.  For example, with a ROC value at `0.773` and `0.768` for the k-NN and logistic regression models still include a lot of error that may cause harm to a person who is predicted as having cardiovascular disease but does not.  Therefore, while these models show improved performance, the k-NN and logistic regression models may not provide valuable predictions based on the ROC values.  Stated differently, predicting any, single person as having cardiovascular disease but does not provides the potential for that person to incur harm, and thus, would likely need improvement before implementation and deployment.  Parameter adjustments to improve performance are discussed below in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the smoking dataset, the classification models include a random forest, decision tree, and gradient boosting, or support vector machines (SVC) algorithm.  Initially, the smoking dataset is up sampled using the “SMOTE” library as above discussed.  This allows for a further model performance improvement and increased accuracy.  The attributes are then split into numerical and categorical features.  This allows for the pre-processing discussed above, such as imputing a median value, to be done through pipelines, which provide an efficient technique for processing data.  Using these pipelines, the smoking dataset is passed through to the classification algorithms, in this case random forest, decision tree, and gradient boosting algorithms.  Again, performance of these algorithms is evaluated against the recall metric to understand the false-negative rate of each of the algorithms.  Again, the higher the recall value, a lower number of false negatives are present in the prediction.  Applying this metric to each of the classification models determines which of the models have the greatest accuracy when predicting the presence of cardiovascular disease.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the 10-fold, stratified cross validation, each of the classification models calculated a recall value.  Shown in the below cell, the recall for the random forest algorithm is `0.898`, the recall for the decision tree algorithm is 0.902, and the recall for the gradient boosting algorithm is 0.910.  Therefore, in comparison, the gradient boosting model provides the greatest recall value between the competing models and provides the greatest indication of accurately predicting whether a person is a smoker.  This suggests that the most appropriate model to predict whether a person is a smoker is an iterative procedure that updates weights of not well classified instances.  The next closest performing model is the decision tree model.  The recall from the decision tree model is only slightly less than the recall for the gradient boosting model, while the random forest model is only slightly less than the decision tree model.  This suggests that, while the recall values suggest that the gradient boosting model is the most appropriate, all of the models, random forest, decision tree, and gradient boosting, are all competitive.  For a prediction of whether a person is a smoker, each of these recall values may have secondary effects, which may impact deployment, such as computation time.  Since the harm to a person to be wrongly classified as a smoker is one of inconvenience, for example needing to submit a physical, additional paperwork, or higher fees, and can be easily corrected, other factors may instruct deployment.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the random forest, decision tree, and gradient boosting models all have recall values greater than `0.88`.  This high recall for each of the competing models suggest that few false positives are present in the prediction.  This provides valuable predictions to identify a person who may require higher health care costs by being a smoker, and thus subjected to higher health insurance premiums.  For example, with a recall value at `0.910` and `0.902`, the gradient boosting and decision tree models, respectively, perform well in accurately predicting a person as a smoker with minimizing the predictions of non-smokers as smokers.  Therefore, while these models show improved performance, the random forest, gradient boosting, and decision tree models may provide valuable predictions based on the recall values.  Parameter adjustments may also aid to further improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand and evaluate the smoking dataset, a comparison is performed between the recall value for the up sampled smoking data against the imbalanced smoking data.  As shown, predictions using the entire smoking dataset and the “SMOTE” library with the decision tree model are improved.  The recall value using the entire smoking dataset is `0.926` for the decision tree model.  This shows an improvement over the initial recall value of `0.902`.  Similarly, providing a decision tree model using the imbalanced data show that a large decrease in the recall value.  As shown, the recall value for the imbalanced smoking data is `0.344`.  This comparison shows that without up sampling the imbalanced smoking dataset, the performance is significantly impacted.  Therefore, model evaluation will use the up sampled smoking dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of each of the models for both of the cardiovascular disease classification problem, and the smoking classification problem may be adjusted, or tuned, to increase generalization performance for both ROC in the cardiovascular disease dataset and recall in the smoking dataset.  individual parameters for each of the models may be tuned to improve performance.  For example, the below cell executes the “GridSearchCV()” function from the “scikit-learn” Python package to tune model parameters to improve performance of each of the models.  The “GridSearchCV()” function compares the possible combinations of the parameters to achieve the best performing model.  Specifically, the “GridSearchCV()” function uses a “fit” and a “score” analysis to find estimators for optimized, specified parameter values.  (https://scikit-learn.org/stable/modules/grid_search.html#grid-search.).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 4 (10 Points)\n",
    "\n",
    "\n",
    "Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We need to have good visualizations here, based on feedback during class. Consider using Yellowbrick.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 5 (10 Points)\n",
    "\n",
    "\n",
    "Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If use ROC, that itself it sufficient to show statistical significance/difference. If something different, look at DataMiningNotebook 6 (Drew’s example). Take a look at MLExtend as well.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (CVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes compared to KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "t statistic: -35.075\np value: 0.000\n"
    }
   ],
   "source": [
    "t, p = paired_ttest_5x2cv(estimator1=clf1, # MultinomialNB\n",
    "                          estimator2=clf2, # KNeighborsClassifier\n",
    "                          X=X_cvd, y=y_cardio,\n",
    "                          random_seed=1)\n",
    "\n",
    "print('t statistic: %.3f' % t)\n",
    "print('p value: %.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reject the null hypothesis that the models perform equally well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes compared to LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "t statistic: -75.365\np value: 0.000\n"
    }
   ],
   "source": [
    "t, p = paired_ttest_5x2cv(estimator1=clf1, # MultinomialNB\n",
    "                          estimator2=clf5, # LogisticRegression\n",
    "                          X=X_cvd, y=y_cardio,\n",
    "                          random_seed=1)\n",
    "\n",
    "print('t statistic: %.3f' % t)\n",
    "print('p value: %.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reject the null hypothesis that the models perform equally well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN compared to LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "t statistic: 5.100\np value: 0.004\n"
    }
   ],
   "source": [
    "t, p = paired_ttest_5x2cv(estimator1=clf2, # KNeighborsClassifier\n",
    "                          estimator2=clf5, # LogisticRegression\n",
    "                          X=X_cvd, y=y_cardio,\n",
    "                          random_seed=1)\n",
    "\n",
    "print('t statistic: %.3f' % t)\n",
    "print('p value: %.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reject the null hypothesis that the models perform equally well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (Smoker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# clf3 = RandomForestClassifier(random_state=1, n_estimators=200, min_samples_split=10, min_samples_leaf=4, max_features='sqrt', bootstrap=True)\n",
    "# clf4 = GradientBoostingClassifier(random_state=1, n_estimators=8, min_samples_split=0.30000000000000004, min_samples_leaf=0.2, max_features=2, max_depth=15.0, loss='exponential')\n",
    "# clf6 = DecisionTreeClassifier(random_state=1, splitter='random', min_samples_split=5, min_samples_leaf=4, max_features='auto', criterion='entropy', class_weight=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest compared to Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "t statistic: 1.691\np value: 0.152\n"
    }
   ],
   "source": [
    "t, p = paired_ttest_5x2cv(estimator1=clf3, # RandomForestClassifier\n",
    "                          estimator2=clf6, # DecisionTreeClassifier\n",
    "                          X=X_smk, y=y_smoke,\n",
    "                          random_seed=1)\n",
    "\n",
    "print('t statistic: %.3f' % t)\n",
    "print('p value: %.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fail to Reject, and we may conclude that the performance of the two algorithms is not significantly different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest compared to Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "t statistic: 16.845\np value: 0.000\n"
    }
   ],
   "source": [
    "t, p = paired_ttest_5x2cv(estimator1=clf3, # RandomForestClassifier\n",
    "                          estimator2=clf4, # GradientBoostingClassifier\n",
    "                          X=X_smk, y=y_smoke,\n",
    "                          random_seed=1)\n",
    "\n",
    "print('t statistic: %.3f' % t)\n",
    "print('p value: %.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reject the null hypothesis that the models perform equally well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree compared to Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "t statistic: 4.816\np value: 0.005\n"
    }
   ],
   "source": [
    "t, p = paired_ttest_5x2cv(estimator1=clf6, # DecisionTreeClassifier\n",
    "                          estimator2=clf4, # GradientBoostingClassifier\n",
    "                          X=X_smk, y=y_smoke,\n",
    "                          random_seed=1)\n",
    "\n",
    "print('t statistic: %.3f' % t)\n",
    "print('p value: %.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reject the null hypothesis that the models perform equally well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 6 (10 Points)\n",
    "\n",
    "\n",
    "Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*- Need to add the coding from the Mini Lab – Interpret feature importance- and similar analysis for the smoking dataset – This was copied directly and not edited yet.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the parameters found from the GridSearch() function, a logistic regression model is executed in the below cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph depicts feature weights for each of the features in the logistic regression model.  Based on the above graph, the features that contribute significantly to this model include “Age,” “Cholesterol,” “Blood Pressure” and the” BMI Group”. Other attributes, like “Gender,” “Smoking,” “Alcohol” and “Activity” don't appear to as strong of indicators for the presence of cardiovascular disease.  For example, each of the features “Age,” “Cholesterol, “Blood Pressure” and the” BMI Group” have strong weights that positively extend to around 0.5.  Likewise, some of the remaining features have some positive extending weights, such as “Smoking,” “Alcohol” and “Activity.”  These weights, however, only appear to extend to around 0.1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using common domain knowledge, the features “Age,” “Cholesterol,” “Blood Pressure” and the” BMI Group” appear to be consistent with attributes that would contribute significantly to predicting the presence of cardiovascular disease.  For example, as a person ages, so does the heart tissue, which would likely result in more cardiovascular injuries.  Similarly, a high cholesterol value is indicative of a high presence of a substance commonly known to cause artery blockage. Lastly, a similar analysis applies to the “Blood Pressure” and the” BMI Group” features.  As the value in each of those features increases, the likelihood of cardiovascular injury also increases, for example through increased arterial pressure or a high body mass.  The other positively extending features also depict that some behaviors, such as smoking and alcohol intake, and inactivity may contribute slightly to the presence of cardiovascular disease.  This also matches the common domain knowledge of cardiovascular disease causes.  The below cell executes a further logistic regression model using the 4, strongest, weighted features to analyze changes in model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment (5 Points)\n",
    "\n",
    "\n",
    "How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cardiovascular disease dataset, the models analyzed do not achieve ROC AUC values greater than 0.783 using gradient boosting as above described.  For the smoking dataset, the models analyzed do not achieve recall values greater than 0.71.  This suggests that the models would not be very useful for interested parties. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cardiovascular disease dataset, the ROC AUC values indicate that there is a relatively high amount of false-positive results.  Specifically, approximately 30% of the data is indicative of a false positive result from using this model.  This equates to treating cardiovascular disease in which 30% of the patients likely do not have cardiovascular disease.  With such a large percentage and harmful results, it is unlikely that the model used to predict cardiovascular disease would not be adopted or deployed.  If the model for the cardiovascular disease dataset were to be deployed, the value of the model would be measured by the number of patients that do not have cardiovascular disease but were treated for cardiovascular disease.  The model used for the cardiovascular disease dataset would be deployed inside a hospital setting, and accessible at a nurse’s station.  This would allow a nurse to input the relevant data, run the model, and provide results to a doctor for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other data that may be collected to improve the performance of the model may include such measurements as commonly used for diagnosing cardiovascular disease.  These may include lipids, proteins or other, known causes of cardiovascular disease. (https://www.ahajournals.org/doi/full/10.1161/01.CIR.0000114134.03187.7B).  The model would need to be updated every time a person suffering cardiovascular disease is admitted or visits the hospital.  This would provide more data for the model to evaluate and predict and may improve usefulness.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the smoking dataset, the recall values also indicate that a substantial percentage, approximately 30%, of the predictions include false positive predictions.  This would equate to approximately 30% of customers being labeled smokers who are not smokers, and thus charged higher premiums.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charging customers for unnecessary expenses increase the cost to the consumer and makes it more likely the consumer will choose another insurance company.  Therefore, the model used to predict a smoker is unlikely to be adopted or deployed by health insurance companies.  If the model were to be deployed, the value of the model would be measured by the number of customers who change insurers based on high premium costs.  This would indicate non-smokers being charged more since those customers are more likely to get a better, competing rate.  The model for the smoking dataset would be deployed at a health insurance company and applied when a new application is received by the company.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other data that may be collected to improve the performance of the model may include the presence of health problems commonly associated with lung disease that are known to be caused by smoking.  These may include the presence of tar on lungs, or a diagnosis of COPD or other respiratory illness caused by smoking.  (https://www.cdc.gov/tobacco/data_statistics/fact_sheets/health_effects/effects_cig_smoking/index.htm#:~:text=Smoking%20can%20cause%20lung%20disease,alveoli)%20found%20in%20your%20lungs.&text=Lung%20diseases%20caused%20by%20smoking,includes%20emphysema%20and%20chronic%20bronchitis.&text=Cigarette%20smoking%20causes%20most%20cases%20of%20lung%20cancer.) The model would need to be updated every time a person applied for health insurance from the company.  This would provide more data for the model to evaluate and predict and may improve usefulness.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exceptional Work (10 points)\n",
    "\n",
    "\n",
    "You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (CVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to use all of data\n",
    "X = X_cvd\n",
    "y = y_cardio\n",
    "\n",
    "numeric_features = ['age', 'cholesterol', 'bp', 'bmiGrp', 'gluc']\n",
    "categorical_features = ['gender', 'smoke', 'alco', 'active']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"median\"))])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"median\")),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "clf1 = MultinomialNB()\n",
    "clf2 = KNeighborsClassifier()\n",
    "clf5 = LogisticRegression(random_state=1)\n",
    "\n",
    "pipe1 = Pipeline([['preprocessor', preprocessor], \n",
    "                 # ['rs', RobustScaler()],             <<< ValueError: Negative values in data passed to MultinomialNB (input X)\n",
    "                 ['clf', clf1]])\n",
    "\n",
    "pipe2 = Pipeline([['preprocessor', preprocessor], \n",
    "                  ['rs', RobustScaler()],\n",
    "                  ['clf', clf2]])\n",
    "\n",
    "pipe5 = Pipeline([['preprocessor', preprocessor], \n",
    "                  ['rs', RobustScaler()],\n",
    "                  ['clf', clf5]])\n",
    "\n",
    "model_params = {\n",
    "    \"multinomialnb\": {\n",
    "        \"model\": pipe1,\n",
    "        \"params\": {\n",
    "            \"clf__alpha\": [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "        }\n",
    "    }\n",
    "    # \"kneighborsClassifier\": {\n",
    "    #     \"model\": pipe2,\n",
    "    #     \"params\": {\n",
    "    #         \"clf__n_neighbors\": np.arange(5,51),\n",
    "    #         \"clf__weights\": [\"uniform\", \"distance\"]\n",
    "    #     }\n",
    "    # },\n",
    "    # \"logisticregression\": {\n",
    "    #     \"model\": pipe5,\n",
    "    #     \"params\": {\n",
    "    #         \"clf__C\": [.01, .1, 1, 5, 10, 25, 50],\n",
    "    #         \"clf__penalty\": [\"l1\", \"l2\"]\n",
    "    #     }\n",
    "    # }\n",
    "}\n",
    "scores = []\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    start = time.time()\n",
    "    clf = GridSearchCV(estimator = mp[\"model\"], param_grid=mp[\"params\"], cv=2, scoring=\"roc_auc\", n_jobs=-1)\n",
    "    # clf = RandomizedSearchCV(estimator = mp[\"model\"], param_distributions=mp[\"params\"], cv=10, scoring=\"roc_auc\", n_jobs=-1)\n",
    "    clf.fit(X, y)\n",
    "    elapsed_time = (time.time() - start)\n",
    "\n",
    "    scores.append({\"Model\": model_name,\n",
    "    \"Best ROC AUC\": clf.best_score_, # Mean cross-validated score of the best_estimator\n",
    "    \"Best Params\": clf.best_params_,\n",
    "    \"results\": clf.cv_results_,\n",
    "    \"Cross Validation Time\": elapsed_time,\n",
    "    \"Best Estimator\": clf.best_estimator_\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10 Fold Cross Validation Scores (CVD):\n\nModel :  multinomialnb\nBest ROC AUC :  0.703235931383499\nBest Params :  {'clf__alpha': 1.0}\nMean Fit Time:  0.0708773374557495\nMean Score Time:  0.04228518009185791\nCross Validation Time :  0.5038347244262695\nPrediction Accuracy :  0.6558677260775123\n"
    }
   ],
   "source": [
    "print('10 Fold Cross Validation Scores (CVD):')\n",
    "\n",
    "for model in scores:\n",
    "    print()\n",
    "    for key, value in model.items():\n",
    "        if key == 'Best Estimator':\n",
    "            print(\"Prediction Accuracy\",': ',value.score(X, y))\n",
    "        elif key == 'results':\n",
    "            print('Mean Fit Time: ', value['mean_fit_time'].mean())\n",
    "            print('Mean Score Time: ', value['mean_score_time'].mean())\n",
    "        else:\n",
    "            print(key,': ',value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (Smoker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to use all of data\n",
    "X = X_smk\n",
    "y = y_smoke\n",
    "\n",
    "numeric_features = ['age', 'cholesterol', 'bp', 'bmiGrp', 'gluc']\n",
    "categorical_features = ['gender', 'cardio', 'alco', 'active']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"median\"))])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"median\")),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "\n",
    "clf3 = RandomForestClassifier(random_state=1)\n",
    "clf4 = GradientBoostingClassifier(random_state=1)\n",
    "clf6 = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "\n",
    "                  \n",
    "pipe3 = Pipeline([['preprocessor', preprocessor], \n",
    "                  ['rs', RobustScaler()],\n",
    "                  ['smt', smt],\n",
    "                  ['clf', clf3]])\n",
    "\n",
    "# 10 Fold Cross Validation Scores:\n",
    "\n",
    "# Model :  randomforestclassifier\n",
    "# Best Recall :  0.679364436212086\n",
    "# Best Params :  {'clf__n_estimators': 200, 'clf__min_samples_split': 10, 'clf__min_samples_leaf': 4, 'clf__max_features': 'sqrt', 'clf__bootstrap': True}\n",
    "# Mean Fit Time:  133.9856176996231\n",
    "# Mean Score Time:  1.8180141115188602\n",
    "# Cross Validation Time :  3429.325078725815\n",
    "# Prediction Accuracy :  0.8127643763576083\n",
    "\n",
    "pipe4 = Pipeline([['preprocessor', preprocessor], \n",
    "                  ['rs', RobustScaler()],\n",
    "                  ['smt', smt],\n",
    "                  ['clf', clf4]])\n",
    "\n",
    "# 10 Fold Cross Validation Scores:\n",
    "\n",
    "# Model :  gradientboostingclassifier\n",
    "# Best Recall :  0.86821233871477\n",
    "# Best Params :  {'clf__n_estimators': 8, 'clf__min_samples_split': 0.30000000000000004, 'clf__min_samples_leaf': 0.2, 'clf__max_features': 2, 'clf__max_depth': 15.0, 'clf__loss': 'exponential'}\n",
    "# Mean Fit Time:  0.899744851589203\n",
    "# Mean Score Time:  0.018294248580932617\n",
    "# Cross Validation Time :  24.21505618095398\n",
    "# Prediction Accuracy :  0.7152738081627986\n",
    "\n",
    "\n",
    "pipe6 = Pipeline([['preprocessor', preprocessor], \n",
    "                  ['rs', RobustScaler()],\n",
    "                  ['smt', smt],\n",
    "                  ['clf', clf6]])\n",
    "\n",
    "\n",
    "# 10 Fold Cross Validation Scores:\n",
    "\n",
    "# Model :  decisiontreeclassifier\n",
    "# Best Recall :  0.8435770064619336\n",
    "# Best Params :  {'clf__splitter': 'random', 'clf__min_samples_split': 5, 'clf__min_samples_leaf': 4, 'clf__max_features': 'auto', 'clf__criterion': 'entropy', 'clf__class_weight': None}\n",
    "# Mean Fit Time:  0.5260472869873046\n",
    "# Mean Score Time:  0.016144995689392087\n",
    "# Cross Validation Time :  14.461254119873047\n",
    "# Prediction Accuracy :  0.7497570595632789\n",
    "\n",
    "model_params = {\n",
    "    # \"randomforestclassifier\": {\n",
    "    #     \"model\": pipe3,\n",
    "    #     \"params\": {\n",
    "    #         \"clf__n_estimators\": [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
    "    #         # \"clf__criterion\": ['gini','entropy'],\n",
    "    #         \"clf__max_features\": ['auto', 'sqrt'],\n",
    "    #         \"clf__min_samples_split\": [2, 5, 10],\n",
    "    #         \"clf__min_samples_leaf\": [1, 2, 4],\n",
    "    #         \"clf__bootstrap\": [True, False]\n",
    "    #     }\n",
    "    # }, \n",
    "    # \"gradientboostingclassifier\": {\n",
    "    #     \"model\": pipe4,\n",
    "    #     \"params\": {\n",
    "    #         \"clf__loss\": ['deviance', 'exponential'],\n",
    "    #         \"clf__n_estimators\": [1, 2, 4, 8, 16, 32, 64, 100, 200],\n",
    "    #         \"clf__max_depth\": np.linspace(1, 32, 32, endpoint=True),\n",
    "    #         \"clf__min_samples_split\": np.linspace(0.1, 1.0, 10, endpoint=True),\n",
    "    #         \"clf__min_samples_leaf\": np.linspace(0.1, 0.5, 5, endpoint=True),\n",
    "    #         \"clf__max_features\": list(range(1,X.shape[1]))\n",
    "    #     }\n",
    "    # }, \n",
    "    \"decisiontreeclassifier\": {\n",
    "        \"model\": pipe6,\n",
    "        \"params\": {\n",
    "            \"clf__criterion\": ['gini','entropy'],\n",
    "            \"clf__splitter\": ['best', 'random'],\n",
    "            \"clf__min_samples_split\": [2, 5, 10],\n",
    "            \"clf__min_samples_leaf\": [1, 2, 4],\n",
    "            \"clf__max_features\": ['auto', 'sqrt', 'log2'],\n",
    "            \"clf__class_weight\": [None, 'balanced']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "scores = []\n",
    "\n",
    "for model_name, mp in model_params.items():\n",
    "    start = time.time()\n",
    "    # clf = GridSearchCV(estimator = mp[\"model\"], param_grid=mp[\"params\"], cv=3, scoring=\"roc_auc\", n_jobs=-1)\n",
    "    clf = RandomizedSearchCV(estimator = mp[\"model\"], param_distributions=mp[\"params\"], cv=2, scoring=\"recall\", n_jobs=-1)\n",
    "    clf.fit(X, y)\n",
    "    elapsed_time = (time.time() - start)\n",
    "\n",
    "    scores.append({\"Model\": model_name,\n",
    "    \"Best Recall\": clf.best_score_, # Mean cross-validated score of the best_estimator\n",
    "    \"Best Params\": clf.best_params_,\n",
    "    \"results\": clf.cv_results_,\n",
    "    \"Cross Validation Time\": elapsed_time,\n",
    "    \"Best Estimator\": clf.best_estimator_\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10 Fold Cross Validation Scores (Smoker):\n\nModel :  decisiontreeclassifier\nBest Recall :  0.8331969048174612\nBest Params :  {'clf__splitter': 'random', 'clf__min_samples_split': 10, 'clf__min_samples_leaf': 4, 'clf__max_features': 'sqrt', 'clf__criterion': 'gini', 'clf__class_weight': None}\nMean Fit Time:  0.2594144821166992\nMean Score Time:  0.04728457927703857\nCross Validation Time :  2.1133065223693848\nPrediction Accuracy :  0.7259631873785298\n"
    }
   ],
   "source": [
    "print('10 Fold Cross Validation Scores (Smoker):')\n",
    "\n",
    "for model in scores:\n",
    "    print()\n",
    "    for key, value in model.items():\n",
    "        if key == 'Best Estimator':\n",
    "            print(\"Prediction Accuracy\",': ',value.score(X, y))\n",
    "        elif key == 'results':\n",
    "            print('Mean Fit Time: ', value['mean_fit_time'].mean())\n",
    "            print('Mean Score Time: ', value['mean_score_time'].mean())\n",
    "        else:\n",
    "            print(key,': ',value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(X_cvd, y_cardio,\n",
    "    test_size=0.2,\n",
    "    random_state=1,\n",
    "    stratify=y)\n",
    "\n",
    "numeric_features = ['age', 'cholesterol', 'bp', 'bmiGrp', 'gluc']\n",
    "categorical_features = ['gender', 'smoke', 'alco', 'active']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"median\"))])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy=\"median\")),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "\n",
    "# nb = MultinomialNB(alpha=1.0)\n",
    "# knn = KNeighborsClassifier(n_neighbors=38)\n",
    "# rf = RandomForestClassifier(random_state=1, n_estimators=200, min_samples_split=10, min_samples_leaf=4, max_features='sqrt', bootstrap=True)\n",
    "# gb = GradientBoostingClassifier(random_state=1, n_estimators=200, min_samples_split=0.4, min_samples_leaf=0.1, max_features=5, max_depth=21.0)\n",
    "logreg = LogisticRegression(random_state=1, C=.01)\n",
    "\n",
    "\n",
    "pipe1 = Pipeline([['preprocessor', preprocessor], \n",
    "                 ['rs', RobustScaler()],\n",
    "                 ['gradientboosting', logreg]])\n",
    "\n",
    "fit = pipe1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n           0       0.71      0.70      0.70      6948\n           1       0.71      0.73      0.72      7048\n\n    accuracy                           0.71     13996\n   macro avg       0.71      0.71      0.71     13996\nweighted avg       0.71      0.71      0.71     13996\n\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 1440x360 with 3 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"331.674375pt\" version=\"1.1\" viewBox=\"0 0 1023.422443 331.674375\" width=\"1023.422443pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 331.674375 \r\nL 1023.422443 331.674375 \r\nL 1023.422443 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 34.240625 294.118125 \r\nL 306.040625 294.118125 \r\nL 306.040625 22.318125 \r\nL 34.240625 22.318125 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#pf90ac678d3)\">\r\n    <image height=\"272\" id=\"image85c61caadc\" transform=\"scale(1 -1)translate(0 -272)\" width=\"272\" x=\"34.240625\" xlink:href=\"data:image/png;base64,\r\niVBORw0KGgoAAAANSUhEUgAAARAAAAEQCAYAAAB4CisVAAAABHNCSVQICAgIfAhkiAAAA3RJREFUeJzt1jERAkEABEEOGRjBAAIIcIK6d0GAARJsPAo+meSOqm4FG03tuI3HfoID2/c1ewILO88eAPwvAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQEyAQGy8f5c9tkjWNfzep89gYV5IEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIEAmIED2A6A7Ce6wnGUfAAAAAElFTkSuQmCC\" y=\"-22.118125\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m84668234c1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"102.190625\" xlink:href=\"#m84668234c1\" y=\"294.118125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(99.009375 308.716563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"238.090625\" xlink:href=\"#m84668234c1\" y=\"294.118125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 1 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(234.909375 308.716563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_3\">\r\n     <!-- Predicted label -->\r\n     <defs>\r\n      <path d=\"M 19.671875 64.796875 \r\nL 19.671875 37.40625 \r\nL 32.078125 37.40625 \r\nQ 38.96875 37.40625 42.71875 40.96875 \r\nQ 46.484375 44.53125 46.484375 51.125 \r\nQ 46.484375 57.671875 42.71875 61.234375 \r\nQ 38.96875 64.796875 32.078125 64.796875 \r\nz\r\nM 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.34375 72.90625 50.609375 67.359375 \r\nQ 56.890625 61.8125 56.890625 51.125 \r\nQ 56.890625 40.328125 50.609375 34.8125 \r\nQ 44.34375 29.296875 32.078125 29.296875 \r\nL 19.671875 29.296875 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-80\"/>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n      <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n      <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n      <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n      <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n      <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      <path id=\"DejaVuSans-32\"/>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\nM 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-98\"/>\r\n     </defs>\r\n     <g transform=\"translate(132.988281 322.394687)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-80\"/>\r\n      <use x=\"58.552734\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"97.416016\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"158.939453\" xlink:href=\"#DejaVuSans-100\"/>\r\n      <use x=\"222.416016\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"250.199219\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"305.179688\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"344.388672\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"405.912109\" xlink:href=\"#DejaVuSans-100\"/>\r\n      <use x=\"469.388672\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"501.175781\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"528.958984\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"590.238281\" xlink:href=\"#DejaVuSans-98\"/>\r\n      <use x=\"653.714844\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"715.238281\" xlink:href=\"#DejaVuSans-108\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_3\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m618fa9c1e4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m618fa9c1e4\" y=\"90.268125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(20.878125 94.067344)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#m618fa9c1e4\" y=\"226.168125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 1 -->\r\n      <g transform=\"translate(20.878125 229.967344)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_6\">\r\n     <!-- True label -->\r\n     <defs>\r\n      <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n      <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n     </defs>\r\n     <g transform=\"translate(14.798438 182.517344)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-84\"/>\r\n      <use x=\"46.333984\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"87.447266\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"150.826172\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"212.349609\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"244.136719\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"271.919922\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"333.199219\" xlink:href=\"#DejaVuSans-98\"/>\r\n      <use x=\"396.675781\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"458.199219\" xlink:href=\"#DejaVuSans-108\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 34.240625 294.118125 \r\nL 34.240625 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 306.040625 294.118125 \r\nL 306.040625 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 34.240625 294.118125 \r\nL 306.040625 294.118125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 34.240625 22.318125 \r\nL 306.040625 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_7\">\r\n    <!-- 0.7 -->\r\n    <defs>\r\n     <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n     <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n    </defs>\r\n    <g style=\"fill:#440154;\" transform=\"translate(94.239063 93.0275)scale(0.1 -0.1)\">\r\n     <use xlink:href=\"#DejaVuSans-48\"/>\r\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n     <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_8\">\r\n    <!-- 0.3 -->\r\n    <defs>\r\n     <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n    </defs>\r\n    <g style=\"fill:#fde725;\" transform=\"translate(230.139062 93.0275)scale(0.1 -0.1)\">\r\n     <use xlink:href=\"#DejaVuSans-48\"/>\r\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n     <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_9\">\r\n    <!-- 0.27 -->\r\n    <defs>\r\n     <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n    </defs>\r\n    <g style=\"fill:#fde725;\" transform=\"translate(91.057813 228.9275)scale(0.1 -0.1)\">\r\n     <use xlink:href=\"#DejaVuSans-48\"/>\r\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n     <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n     <use x=\"159.033203\" xlink:href=\"#DejaVuSans-55\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_10\">\r\n    <!-- 0.73 -->\r\n    <g style=\"fill:#440154;\" transform=\"translate(226.957812 228.9275)scale(0.1 -0.1)\">\r\n     <use xlink:href=\"#DejaVuSans-48\"/>\r\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n     <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\r\n     <use x=\"159.033203\" xlink:href=\"#DejaVuSans-51\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_11\">\r\n    <!-- Confusion Matrix -->\r\n    <defs>\r\n     <path d=\"M 64.40625 67.28125 \r\nL 64.40625 56.890625 \r\nQ 59.421875 61.53125 53.78125 63.8125 \r\nQ 48.140625 66.109375 41.796875 66.109375 \r\nQ 29.296875 66.109375 22.65625 58.46875 \r\nQ 16.015625 50.828125 16.015625 36.375 \r\nQ 16.015625 21.96875 22.65625 14.328125 \r\nQ 29.296875 6.6875 41.796875 6.6875 \r\nQ 48.140625 6.6875 53.78125 8.984375 \r\nQ 59.421875 11.28125 64.40625 15.921875 \r\nL 64.40625 5.609375 \r\nQ 59.234375 2.09375 53.4375 0.328125 \r\nQ 47.65625 -1.421875 41.21875 -1.421875 \r\nQ 24.65625 -1.421875 15.125 8.703125 \r\nQ 5.609375 18.84375 5.609375 36.375 \r\nQ 5.609375 53.953125 15.125 64.078125 \r\nQ 24.65625 74.21875 41.21875 74.21875 \r\nQ 47.75 74.21875 53.53125 72.484375 \r\nQ 59.328125 70.75 64.40625 67.28125 \r\nz\r\n\" id=\"DejaVuSans-67\"/>\r\n     <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n     <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n     <path d=\"M 37.109375 75.984375 \r\nL 37.109375 68.5 \r\nL 28.515625 68.5 \r\nQ 23.6875 68.5 21.796875 66.546875 \r\nQ 19.921875 64.59375 19.921875 59.515625 \r\nL 19.921875 54.6875 \r\nL 34.71875 54.6875 \r\nL 34.71875 47.703125 \r\nL 19.921875 47.703125 \r\nL 19.921875 0 \r\nL 10.890625 0 \r\nL 10.890625 47.703125 \r\nL 2.296875 47.703125 \r\nL 2.296875 54.6875 \r\nL 10.890625 54.6875 \r\nL 10.890625 58.5 \r\nQ 10.890625 67.625 15.140625 71.796875 \r\nQ 19.390625 75.984375 28.609375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-102\"/>\r\n     <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     <path d=\"M 9.8125 72.90625 \r\nL 24.515625 72.90625 \r\nL 43.109375 23.296875 \r\nL 61.8125 72.90625 \r\nL 76.515625 72.90625 \r\nL 76.515625 0 \r\nL 66.890625 0 \r\nL 66.890625 64.015625 \r\nL 48.09375 14.015625 \r\nL 38.1875 14.015625 \r\nL 19.390625 64.015625 \r\nL 19.390625 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-77\"/>\r\n     <path d=\"M 54.890625 54.6875 \r\nL 35.109375 28.078125 \r\nL 55.90625 0 \r\nL 45.3125 0 \r\nL 29.390625 21.484375 \r\nL 13.484375 0 \r\nL 2.875 0 \r\nL 24.125 28.609375 \r\nL 4.6875 54.6875 \r\nL 15.28125 54.6875 \r\nL 29.78125 35.203125 \r\nL 44.28125 54.6875 \r\nz\r\n\" id=\"DejaVuSans-120\"/>\r\n    </defs>\r\n    <g transform=\"translate(119.49875 16.318125)scale(0.12 -0.12)\">\r\n     <use xlink:href=\"#DejaVuSans-67\"/>\r\n     <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"131.005859\" xlink:href=\"#DejaVuSans-110\"/>\r\n     <use x=\"194.384766\" xlink:href=\"#DejaVuSans-102\"/>\r\n     <use x=\"229.589844\" xlink:href=\"#DejaVuSans-117\"/>\r\n     <use x=\"292.96875\" xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"345.068359\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"372.851562\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"434.033203\" xlink:href=\"#DejaVuSans-110\"/>\r\n     <use x=\"497.412109\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"529.199219\" xlink:href=\"#DejaVuSans-77\"/>\r\n     <use x=\"615.478516\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"676.757812\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"715.966797\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"757.080078\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"784.863281\" xlink:href=\"#DejaVuSans-120\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n  <g id=\"axes_2\">\r\n   <g id=\"patch_7\">\r\n    <path d=\"M 508.949716 294.118125 \r\nL 1016.222443 294.118125 \r\nL 1016.222443 22.318125 \r\nL 508.949716 22.318125 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_3\">\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"532.007567\" xlink:href=\"#m84668234c1\" y=\"294.118125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.0 -->\r\n      <g transform=\"translate(524.056005 308.716563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"624.238972\" xlink:href=\"#m84668234c1\" y=\"294.118125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(616.28741 308.716563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"716.470377\" xlink:href=\"#m84668234c1\" y=\"294.118125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 0.4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(708.518815 308.716563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"808.701782\" xlink:href=\"#m84668234c1\" y=\"294.118125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 0.6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(800.75022 308.716563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"900.933187\" xlink:href=\"#m84668234c1\" y=\"294.118125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- 0.8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(892.981624 308.716563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"993.164592\" xlink:href=\"#m84668234c1\" y=\"294.118125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_17\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(985.213029 308.716563)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_18\">\r\n     <!-- False Positive Rate -->\r\n     <defs>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 51.703125 72.90625 \r\nL 51.703125 64.59375 \r\nL 19.671875 64.59375 \r\nL 19.671875 43.109375 \r\nL 48.578125 43.109375 \r\nL 48.578125 34.8125 \r\nL 19.671875 34.8125 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-70\"/>\r\n      <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      <path d=\"M 44.390625 34.1875 \r\nQ 47.5625 33.109375 50.5625 29.59375 \r\nQ 53.5625 26.078125 56.59375 19.921875 \r\nL 66.609375 0 \r\nL 56 0 \r\nL 46.6875 18.703125 \r\nQ 43.0625 26.03125 39.671875 28.421875 \r\nQ 36.28125 30.8125 30.421875 30.8125 \r\nL 19.671875 30.8125 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nL 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.578125 72.90625 50.734375 67.671875 \r\nQ 56.890625 62.453125 56.890625 51.90625 \r\nQ 56.890625 45.015625 53.6875 40.46875 \r\nQ 50.484375 35.9375 44.390625 34.1875 \r\nz\r\nM 19.671875 64.796875 \r\nL 19.671875 38.921875 \r\nL 32.078125 38.921875 \r\nQ 39.203125 38.921875 42.84375 42.21875 \r\nQ 46.484375 45.515625 46.484375 51.90625 \r\nQ 46.484375 58.296875 42.84375 61.546875 \r\nQ 39.203125 64.796875 32.078125 64.796875 \r\nz\r\n\" id=\"DejaVuSans-82\"/>\r\n     </defs>\r\n     <g transform=\"translate(716.119673 322.394687)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-70\"/>\r\n      <use x=\"48.394531\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"109.673828\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"137.457031\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"189.556641\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"251.080078\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"282.867188\" xlink:href=\"#DejaVuSans-80\"/>\r\n      <use x=\"339.544922\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"400.726562\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"452.826172\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"480.609375\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"519.818359\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"547.601562\" xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"606.78125\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"668.304688\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"700.091797\" xlink:href=\"#DejaVuSans-82\"/>\r\n      <use x=\"767.324219\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"828.603516\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"867.8125\" xlink:href=\"#DejaVuSans-101\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_4\">\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"508.949716\" xlink:href=\"#m618fa9c1e4\" y=\"281.76358\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_19\">\r\n      <!-- 0.0 -->\r\n      <g transform=\"translate(486.046591 285.562798)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"508.949716\" xlink:href=\"#m618fa9c1e4\" y=\"232.345398\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_20\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(486.046591 236.144616)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"508.949716\" xlink:href=\"#m618fa9c1e4\" y=\"182.927216\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_21\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(486.046591 186.726435)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"508.949716\" xlink:href=\"#m618fa9c1e4\" y=\"133.509034\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_22\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(486.046591 137.308253)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"508.949716\" xlink:href=\"#m618fa9c1e4\" y=\"84.090852\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_23\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(486.046591 87.890071)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"508.949716\" xlink:href=\"#m618fa9c1e4\" y=\"34.67267\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_24\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(486.046591 38.471889)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_25\">\r\n     <!-- True Positive Rate -->\r\n     <g transform=\"translate(479.966903 202.747813)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-84\"/>\r\n      <use x=\"46.333984\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"87.447266\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"150.826172\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"212.349609\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"244.136719\" xlink:href=\"#DejaVuSans-80\"/>\r\n      <use x=\"300.814453\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"361.996094\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"414.095703\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"441.878906\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"481.087891\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"508.871094\" xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"568.050781\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"629.574219\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"661.361328\" xlink:href=\"#DejaVuSans-82\"/>\r\n      <use x=\"728.59375\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"789.873047\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"829.082031\" xlink:href=\"#DejaVuSans-101\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_17\">\r\n    <path clip-path=\"url(#pfcf3296556)\" d=\"M 532.007567 281.76358 \r\nL 532.07394 281.307822 \r\nL 532.273058 281.307822 \r\nL 532.33943 280.396306 \r\nL 532.538548 280.185956 \r\nL 532.604921 279.940548 \r\nL 532.671293 279.940548 \r\nL 532.671293 279.835373 \r\nL 532.671293 279.765256 \r\nL 532.936784 279.029032 \r\nL 533.003157 278.678449 \r\nL 533.135902 278.292808 \r\nL 533.135902 277.381292 \r\nL 533.268647 277.381292 \r\nL 533.33502 276.925534 \r\nL 533.401392 276.890476 \r\nL 533.60051 276.18931 \r\nL 533.666883 276.119193 \r\nL 533.932373 275.97896 \r\nL 533.998746 275.453085 \r\nL 534.065119 275.312852 \r\nL 534.065119 275.172619 \r\nL 534.131491 275.172619 \r\nL 534.728845 273.91052 \r\nL 534.795218 272.543246 \r\nL 534.927963 272.26278 \r\nL 534.994335 271.982313 \r\nL 536.056298 270.124223 \r\nL 536.056298 269.98399 \r\nL 536.321788 269.878815 \r\nL 536.388161 269.598349 \r\nL 536.520906 269.458116 \r\nL 536.587279 269.177649 \r\nL 536.720024 269.142591 \r\nL 536.720024 268.897183 \r\nL 536.919142 268.862125 \r\nL 536.919142 268.827066 \r\nL 536.985514 268.5466 \r\nL 537.450123 268.231075 \r\nL 537.582868 267.880492 \r\nL 537.582868 267.249443 \r\nL 537.781986 266.968976 \r\nL 537.848358 266.758626 \r\nL 538.37934 264.725245 \r\nL 538.64483 264.62007 \r\nL 538.64483 264.514895 \r\nL 538.711203 264.514895 \r\nL 538.777575 264.374662 \r\nL 538.843948 263.813729 \r\nL 539.242184 262.516572 \r\nL 539.507674 262.236106 \r\nL 539.574047 261.32459 \r\nL 539.640419 261.32459 \r\nL 539.706792 260.798715 \r\nL 539.773165 260.798715 \r\nL 539.773165 260.69354 \r\nL 539.839537 259.957316 \r\nL 539.972283 259.817083 \r\nL 540.038655 259.431441 \r\nL 540.105028 259.431441 \r\nL 540.105028 259.361325 \r\nL 540.1714 259.080858 \r\nL 540.370518 258.870509 \r\nL 540.436891 258.730275 \r\nL 540.569636 258.590042 \r\nL 540.967872 257.994051 \r\nL 541.233363 257.853818 \r\nL 541.299735 256.521602 \r\nL 541.366108 256.311253 \r\nL 541.43248 255.785378 \r\nL 541.631598 255.11927 \r\nL 541.631598 254.488221 \r\nL 541.963461 254.383046 \r\nL 541.963461 253.857172 \r\nL 542.228952 253.716938 \r\nL 542.295325 253.120947 \r\nL 542.361697 253.050831 \r\nL 542.42807 252.524956 \r\nL 542.826306 251.718615 \r\nL 542.892678 251.087566 \r\nL 543.158169 250.246166 \r\nL 543.423659 249.790409 \r\nL 543.490032 249.790409 \r\nL 543.490032 249.685234 \r\nL 543.490032 249.159359 \r\nL 543.622777 248.843834 \r\nL 543.68915 248.31796 \r\nL 543.755522 248.247843 \r\nL 543.821895 247.266211 \r\nL 543.95464 247.231153 \r\nL 544.021013 246.88057 \r\nL 544.153758 246.88057 \r\nL 544.220131 246.740336 \r\nL 544.220131 246.03917 \r\nL 544.419249 245.898937 \r\nL 544.485621 245.478237 \r\nL 544.684739 245.408121 \r\nL 544.751112 244.671897 \r\nL 545.21572 244.426488 \r\nL 545.282093 244.146022 \r\nL 545.414838 244.075905 \r\nL 545.547583 243.725322 \r\nL 545.813074 243.339681 \r\nL 545.813074 243.129331 \r\nL 545.879447 243.129331 \r\nL 546.012192 242.393107 \r\nL 546.078564 241.656883 \r\nL 546.144937 241.656883 \r\nL 546.144937 241.586766 \r\nL 546.21131 241.481591 \r\nL 546.21131 241.341358 \r\nL 546.808663 239.868909 \r\nL 547.273272 239.518326 \r\nL 547.273272 239.23786 \r\nL 547.804253 238.010819 \r\nL 547.870625 237.765411 \r\nL 548.335234 237.309653 \r\nL 548.401606 237.204478 \r\nL 549.131705 236.363079 \r\nL 549.198078 236.222846 \r\nL 549.397196 236.117671 \r\nL 549.529941 235.942379 \r\nL 549.596314 235.416505 \r\nL 549.662686 235.416505 \r\nL 549.662686 235.346388 \r\nL 549.662686 235.171097 \r\nL 549.729059 235.171097 \r\nL 549.99455 234.575106 \r\nL 549.99455 234.504989 \r\nL 550.193667 234.469931 \r\nL 550.193667 234.399814 \r\nL 550.26004 234.154406 \r\nL 550.392785 234.084289 \r\nL 550.392785 234.049231 \r\nL 550.658276 233.628532 \r\nL 550.724648 233.383123 \r\nL 550.923766 232.927366 \r\nL 550.990139 232.927366 \r\nL 551.056512 232.331374 \r\nL 551.122884 232.191141 \r\nL 551.189257 231.910675 \r\nL 551.255629 231.770442 \r\nL 551.587493 231.279625 \r\nL 551.852983 230.052585 \r\nL 552.184846 229.912352 \r\nL 552.251219 229.561769 \r\nL 552.317592 229.246244 \r\nL 552.649455 228.895661 \r\nL 552.715827 228.545078 \r\nL 552.7822 228.545078 \r\nL 552.7822 228.51002 \r\nL 553.180436 227.703679 \r\nL 553.246808 227.563445 \r\nL 553.379554 227.493329 \r\nL 553.578672 227.212862 \r\nL 553.645044 226.967454 \r\nL 553.976907 226.511696 \r\nL 554.04328 225.915705 \r\nL 554.242398 225.845589 \r\nL 554.30877 225.319714 \r\nL 554.375143 225.319714 \r\nL 554.375143 225.249598 \r\nL 554.441516 225.004189 \r\nL 554.640634 225.004189 \r\nL 554.640634 224.934073 \r\nL 554.707006 224.37314 \r\nL 554.773379 224.37314 \r\nL 554.773379 224.232907 \r\nL 555.56985 223.707032 \r\nL 555.56985 223.426566 \r\nL 555.636223 223.426566 \r\nL 556.034459 223.111041 \r\nL 556.233577 222.620225 \r\nL 556.366322 222.585167 \r\nL 556.764558 221.393184 \r\nL 556.897303 221.393184 \r\nL 556.897303 221.323068 \r\nL 556.963676 221.042601 \r\nL 557.030048 220.972485 \r\nL 557.295539 220.166144 \r\nL 557.693774 219.745444 \r\nL 557.760147 219.394861 \r\nL 558.025638 219.184511 \r\nL 558.09201 218.79887 \r\nL 558.158383 218.763812 \r\nL 558.224756 218.518404 \r\nL 558.291128 218.518404 \r\nL 558.556619 217.571829 \r\nL 558.822109 217.466655 \r\nL 558.888482 217.010897 \r\nL 559.153972 216.309731 \r\nL 559.286718 216.204556 \r\nL 559.35309 215.538448 \r\nL 559.35309 215.433273 \r\nL 559.419463 215.433273 \r\nL 559.751326 215.08269 \r\nL 560.149562 214.346466 \r\nL 560.149562 214.101058 \r\nL 560.879661 212.593551 \r\nL 560.879661 212.278026 \r\nL 561.012406 212.137793 \r\nL 561.808877 210.700402 \r\nL 562.472604 208.667021 \r\nL 562.605349 208.667021 \r\nL 562.605349 208.561846 \r\nL 562.671722 208.386554 \r\nL 563.13633 207.229631 \r\nL 563.202703 206.703756 \r\nL 563.468193 206.563523 \r\nL 564.065547 205.967532 \r\nL 564.131919 205.652007 \r\nL 564.264665 205.441657 \r\nL 564.39741 205.196249 \r\nL 564.596528 205.126132 \r\nL 564.596528 205.020958 \r\nL 564.928391 204.880724 \r\nL 565.061136 204.389908 \r\nL 565.127509 204.249675 \r\nL 565.193882 204.249675 \r\nL 565.193882 204.1445 \r\nL 565.260254 204.004267 \r\nL 565.392999 203.828975 \r\nL 565.459372 203.7238 \r\nL 565.525745 203.7238 \r\nL 565.525745 203.653684 \r\nL 565.592117 203.303101 \r\nL 565.724863 202.882401 \r\nL 565.791235 202.847343 \r\nL 565.857608 202.426643 \r\nL 565.92398 202.426643 \r\nL 565.92398 202.321468 \r\nL 566.654079 201.024311 \r\nL 566.786825 201.024311 \r\nL 566.985943 200.463378 \r\nL 567.052315 200.323145 \r\nL 567.317806 199.902446 \r\nL 568.047905 198.14953 \r\nL 568.18065 198.114472 \r\nL 568.379768 197.869064 \r\nL 569.242612 196.466732 \r\nL 569.508102 196.361557 \r\nL 569.574475 196.081091 \r\nL 569.773593 195.204633 \r\nL 569.839966 194.889108 \r\nL 570.304574 194.538525 \r\nL 570.370947 194.43335 \r\nL 570.570064 194.223001 \r\nL 570.901928 193.837359 \r\nL 570.9683 193.451718 \r\nL 570.9683 193.381601 \r\nL 571.167418 193.276427 \r\nL 571.233791 193.101135 \r\nL 571.366536 193.101135 \r\nL 571.432909 192.715494 \r\nL 571.499281 192.715494 \r\nL 571.565654 192.259736 \r\nL 572.096635 191.909153 \r\nL 572.096635 191.803978 \r\nL 572.22938 191.663745 \r\nL 572.295753 191.13787 \r\nL 572.693989 190.997637 \r\nL 572.959479 190.261413 \r\nL 573.025852 189.70048 \r\nL 573.755951 188.999314 \r\nL 573.955069 188.999314 \r\nL 573.955069 188.929197 \r\nL 573.955069 188.788964 \r\nL 574.353304 187.947565 \r\nL 574.419677 187.63204 \r\nL 574.552422 187.63204 \r\nL 574.618795 187.246399 \r\nL 575.282521 186.054416 \r\nL 575.348894 185.703833 \r\nL 575.548012 185.5636 \r\nL 575.548012 185.388309 \r\nL 576.543601 184.056093 \r\nL 576.609974 184.021035 \r\nL 576.609974 183.880802 \r\nL 577.207327 183.109519 \r\nL 577.406445 183.074461 \r\nL 577.472818 182.723878 \r\nL 577.804681 182.618703 \r\nL 577.804681 182.47847 \r\nL 578.202917 182.092828 \r\nL 578.402035 181.952595 \r\nL 578.667525 181.531896 \r\nL 579.132134 181.321546 \r\nL 579.132134 181.216371 \r\nL 579.463997 180.865788 \r\nL 579.928605 180.585321 \r\nL 579.928605 180.269797 \r\nL 580.326841 179.884155 \r\nL 580.459586 179.814039 \r\nL 580.459586 179.638747 \r\nL 580.725077 179.463456 \r\nL 580.791449 179.147931 \r\nL 581.05694 179.042756 \r\nL 581.388803 178.201357 \r\nL 581.720666 177.395016 \r\nL 581.720666 177.219724 \r\nL 581.986157 177.11455 \r\nL 582.052529 177.009375 \r\nL 582.649883 176.623733 \r\nL 582.649883 176.27315 \r\nL 582.915373 176.0628 \r\nL 583.114491 175.887509 \r\nL 583.180864 175.431751 \r\nL 583.910963 173.959302 \r\nL 586.234005 171.084522 \r\nL 586.300377 170.663822 \r\nL 586.499495 170.523589 \r\nL 586.632241 170.243122 \r\nL 586.698613 169.857481 \r\nL 587.096849 169.752306 \r\nL 587.36234 169.121257 \r\nL 587.893321 169.016082 \r\nL 587.959693 168.770674 \r\nL 588.225184 168.525266 \r\nL 588.225184 167.964333 \r\nL 588.62342 167.789041 \r\nL 588.955283 167.753983 \r\nL 588.955283 167.718925 \r\nL 589.353518 167.4034 \r\nL 589.419891 166.947642 \r\nL 589.486264 166.947642 \r\nL 589.486264 166.877525 \r\nL 589.552636 166.526942 \r\nL 589.884499 166.456826 \r\nL 589.884499 166.421768 \r\nL 589.950872 166.246476 \r\nL 590.017245 166.246476 \r\nL 590.614598 165.299902 \r\nL 590.680971 165.019435 \r\nL 591.477443 163.967686 \r\nL 591.610188 163.932628 \r\nL 592.539405 162.775704 \r\nL 592.539405 162.600413 \r\nL 592.67215 162.600413 \r\nL 593.402249 161.127964 \r\nL 593.468621 160.882556 \r\nL 593.601367 160.882556 \r\nL 593.601367 160.812439 \r\nL 594.530583 159.620457 \r\nL 594.596956 159.410107 \r\nL 594.796074 159.199757 \r\nL 595.127937 159.094583 \r\nL 595.260682 158.884233 \r\nL 596.123527 158.568708 \r\nL 596.123527 158.463533 \r\nL 596.72088 157.727309 \r\nL 596.986371 157.622134 \r\nL 597.251861 157.236493 \r\nL 597.650097 157.166376 \r\nL 597.849215 156.640501 \r\nL 598.645686 156.25486 \r\nL 598.712059 156.04451 \r\nL 598.97755 155.974394 \r\nL 599.043922 155.869219 \r\nL 599.110295 155.764044 \r\nL 599.24304 155.764044 \r\nL 599.375785 155.518636 \r\nL 599.442158 155.343344 \r\nL 599.707649 155.308286 \r\nL 599.707649 155.238169 \r\nL 599.774021 155.062878 \r\nL 599.840394 154.957703 \r\nL 599.840394 154.852528 \r\nL 599.973139 154.747353 \r\nL 600.636865 154.011129 \r\nL 601.101474 153.730662 \r\nL 601.366964 153.204788 \r\nL 601.632455 153.134671 \r\nL 601.7652 152.924321 \r\nL 601.897945 152.819147 \r\nL 602.362554 152.258214 \r\nL 602.495299 152.188097 \r\nL 603.225398 151.486931 \r\nL 603.225398 151.381756 \r\nL 603.29177 151.381756 \r\nL 603.424516 151.346698 \r\nL 603.424516 151.031173 \r\nL 604.28736 149.909308 \r\nL 604.486478 149.839191 \r\nL 604.486478 149.804133 \r\nL 604.55285 149.278258 \r\nL 604.751968 149.208142 \r\nL 604.818341 148.997792 \r\nL 605.150204 148.61215 \r\nL 605.482067 148.226509 \r\nL 605.482067 148.121334 \r\nL 606.013048 147.700635 \r\nL 606.013048 147.525343 \r\nL 606.610402 147.279935 \r\nL 606.676775 147.139702 \r\nL 607.605991 146.824177 \r\nL 607.672364 146.719002 \r\nL 608.203345 146.683944 \r\nL 608.203345 146.613827 \r\nL 608.269718 146.543711 \r\nL 608.535208 145.947719 \r\nL 608.535208 145.912661 \r\nL 608.601581 145.912661 \r\nL 608.800699 145.597136 \r\nL 608.867071 145.351728 \r\nL 609.066189 145.10632 \r\nL 609.132562 144.685621 \r\nL 609.33168 144.650562 \r\nL 610.725505 143.107997 \r\nL 610.791878 143.03788 \r\nL 611.057368 142.967764 \r\nL 611.521976 142.652239 \r\nL 611.721094 141.916015 \r\nL 612.451193 141.320024 \r\nL 612.517566 141.039557 \r\nL 612.650311 140.864266 \r\nL 612.783056 140.829207 \r\nL 614.840608 139.391817 \r\nL 614.973353 139.391817 \r\nL 615.437962 138.725709 \r\nL 615.703452 138.51536 \r\nL 616.433551 137.919368 \r\nL 616.632669 137.603844 \r\nL 618.623848 137.148086 \r\nL 618.623848 137.077969 \r\nL 619.221201 136.867619 \r\nL 619.221201 136.692328 \r\nL 620.084046 135.81587 \r\nL 620.681399 135.535404 \r\nL 620.681399 135.325054 \r\nL 620.94689 135.149763 \r\nL 622.075224 134.16813 \r\nL 622.274342 134.027897 \r\nL 622.871696 133.011206 \r\nL 622.938069 132.870973 \r\nL 623.004441 132.485332 \r\nL 623.203559 132.380157 \r\nL 623.203559 132.345098 \r\nL 623.269932 132.345098 \r\nL 623.73454 132.274982 \r\nL 624.597384 131.714049 \r\nL 624.73013 131.714049 \r\nL 624.73013 131.643932 \r\nL 625.261111 131.328408 \r\nL 625.261111 131.258291 \r\nL 625.327483 131.258291 \r\nL 625.460228 130.87265 \r\nL 625.725719 130.802533 \r\nL 625.725719 130.767475 \r\nL 625.792092 130.592183 \r\nL 626.057582 130.101367 \r\nL 626.323073 129.996192 \r\nL 626.389445 129.820901 \r\nL 626.455818 129.715726 \r\nL 626.854054 129.049618 \r\nL 627.185917 128.734093 \r\nL 627.252289 128.558802 \r\nL 627.51778 128.488685 \r\nL 627.51778 128.453627 \r\nL 627.51778 128.38351 \r\nL 628.513369 127.962811 \r\nL 628.579742 127.787519 \r\nL 630.106312 126.981178 \r\nL 631.168275 125.929429 \r\nL 631.832001 125.789196 \r\nL 631.832001 125.754138 \r\nL 631.898373 125.754138 \r\nL 631.964746 125.719079 \r\nL 632.82759 124.772505 \r\nL 634.08867 124.281689 \r\nL 634.752397 123.65064 \r\nL 634.818769 123.44029 \r\nL 635.747986 122.669007 \r\nL 635.814359 122.669007 \r\nL 635.814359 122.528774 \r\nL 636.212594 122.423599 \r\nL 636.212594 122.388541 \r\nL 636.478085 122.213249 \r\nL 637.340929 121.5822 \r\nL 637.87191 121.37185 \r\nL 638.402891 121.266675 \r\nL 638.602009 121.1615 \r\nL 639.265735 120.670684 \r\nL 639.332108 120.355159 \r\nL 639.863089 120.285043 \r\nL 640.128579 119.864343 \r\nL 640.65956 119.022944 \r\nL 640.991424 118.917769 \r\nL 641.721523 118.356836 \r\nL 642.650739 118.146486 \r\nL 643.579956 117.410262 \r\nL 643.646329 117.234971 \r\nL 643.845447 117.234971 \r\nL 644.841036 116.849329 \r\nL 645.305644 116.568863 \r\nL 645.504762 116.568863 \r\nL 645.504762 116.533804 \r\nL 645.836626 116.288396 \r\nL 645.836626 116.113105 \r\nL 647.96055 115.271706 \r\nL 649.088884 115.201589 \r\nL 652.341143 113.483732 \r\nL 652.341143 113.343499 \r\nL 652.805752 113.168208 \r\nL 653.801341 112.186575 \r\nL 655.128794 111.695759 \r\nL 655.925265 111.240001 \r\nL 655.925265 111.134826 \r\nL 656.05801 111.134826 \r\nL 656.05801 111.064709 \r\nL 657.119972 110.468718 \r\nL 657.451836 109.837669 \r\nL 657.916444 109.662377 \r\nL 658.248307 109.171561 \r\nL 658.712915 109.066386 \r\nL 658.712915 109.031328 \r\nL 659.509387 108.820978 \r\nL 659.509387 108.715803 \r\nL 659.57576 108.715803 \r\nL 659.774878 108.36522 \r\nL 659.907623 108.36522 \r\nL 659.907623 108.295104 \r\nL 660.173113 108.15487 \r\nL 660.969585 107.699113 \r\nL 660.969585 107.628996 \r\nL 661.035958 107.628996 \r\nL 661.168703 107.278413 \r\nL 662.164292 107.068063 \r\nL 662.164292 107.033005 \r\nL 662.429783 106.577247 \r\nL 662.894391 106.472072 \r\nL 663.159882 106.261722 \r\nL 663.359 106.191606 \r\nL 663.425372 106.016314 \r\nL 663.491745 106.016314 \r\nL 669.730772 103.807641 \r\nL 669.730772 103.667408 \r\nL 670.129008 103.667408 \r\nL 670.129008 103.562233 \r\nL 670.261753 103.457058 \r\nL 673.182149 102.370251 \r\nL 673.314894 102.265076 \r\nL 673.912247 101.739201 \r\nL 674.841464 101.458735 \r\nL 674.841464 101.318502 \r\nL 675.106955 101.178268 \r\nL 675.969799 100.862744 \r\nL 676.036172 100.617336 \r\nL 676.301662 100.512161 \r\nL 676.301662 100.477102 \r\nL 676.965388 100.231694 \r\nL 677.429997 99.951228 \r\nL 677.429997 99.670761 \r\nL 677.496369 99.670761 \r\nL 678.890195 99.004654 \r\nL 679.155685 98.969595 \r\nL 680.084902 98.689129 \r\nL 680.28402 98.583954 \r\nL 680.947746 98.373604 \r\nL 680.947746 98.233371 \r\nL 681.080491 98.233371 \r\nL 681.080491 98.198313 \r\nL 681.146864 97.952905 \r\nL 681.412355 97.917846 \r\nL 681.412355 97.84773 \r\nL 681.412355 97.777613 \r\nL 681.677845 97.567263 \r\nL 681.744218 97.462088 \r\nL 682.80618 96.936214 \r\nL 683.005298 96.936214 \r\nL 685.527458 96.445398 \r\nL 685.527458 96.410339 \r\nL 685.59383 96.410339 \r\nL 687.319519 95.98964 \r\nL 687.385891 95.849407 \r\nL 687.518636 95.814348 \r\nL 694.620508 93.360267 \r\nL 698.403748 91.922877 \r\nL 698.536493 91.817702 \r\nL 698.602865 91.677469 \r\nL 699.863945 91.221711 \r\nL 699.863945 91.081478 \r\nL 700.859535 90.976303 \r\nL 701.722379 90.590661 \r\nL 702.25336 90.41537 \r\nL 702.51885 90.240078 \r\nL 702.585223 90.20502 \r\nL 702.585223 90.064787 \r\nL 702.717968 90.064787 \r\nL 704.642775 89.468796 \r\nL 706.169345 88.97798 \r\nL 706.235718 88.837746 \r\nL 706.965817 88.76763 \r\nL 707.032189 88.627396 \r\nL 707.164935 88.627396 \r\nL 707.231307 88.241755 \r\nL 714.665042 86.348607 \r\nL 716.39073 85.927907 \r\nL 716.722593 85.717557 \r\nL 719.842107 84.560634 \r\nL 720.704951 84.455459 \r\nL 720.704951 84.4204 \r\nL 720.837696 84.350284 \r\nL 723.95721 83.088185 \r\nL 725.019172 82.842777 \r\nL 725.019172 82.77266 \r\nL 726.213879 82.422077 \r\nL 726.280252 82.316902 \r\nL 727.143096 81.931261 \r\nL 727.408586 81.896203 \r\nL 727.408586 81.861144 \r\nL 728.205058 81.580678 \r\nL 728.205058 81.510561 \r\nL 728.536921 81.440445 \r\nL 728.736039 81.265153 \r\nL 728.935157 81.159978 \r\nL 728.935157 81.12492 \r\nL 730.395355 81.019745 \r\nL 730.395355 80.879512 \r\nL 730.992708 80.774337 \r\nL 732.05467 80.493871 \r\nL 733.249378 80.388696 \r\nL 733.647613 80.283521 \r\nL 733.913104 79.897879 \r\nL 736.634382 79.47718 \r\nL 738.891051 78.84613 \r\nL 739.289287 78.776014 \r\nL 740.616739 78.285198 \r\nL 745.661059 76.98804 \r\nL 745.860177 76.917924 \r\nL 749.908907 75.270184 \r\nL 750.174398 75.270184 \r\nL 752.231949 74.814426 \r\nL 752.364695 74.744309 \r\nL 753.493029 74.358668 \r\nL 753.75852 74.043143 \r\nL 756.811661 73.412094 \r\nL 757.010779 73.341977 \r\nL 757.54176 73.201744 \r\nL 758.935585 72.816103 \r\nL 759.001958 72.710928 \r\nL 759.134703 72.710928 \r\nL 760.926764 72.430461 \r\nL 763.183433 71.589062 \r\nL 763.249806 71.448829 \r\nL 763.382551 71.448829 \r\nL 763.382551 71.41377 \r\nL 764.11265 71.308596 \r\nL 764.909122 70.993071 \r\nL 765.041867 70.993071 \r\nL 765.041867 70.958013 \r\nL 770.086187 70.116613 \r\nL 770.285304 70.081555 \r\nL 774.20129 69.240156 \r\nL 774.20129 69.134981 \r\nL 776.258841 68.924631 \r\nL 776.723449 68.679223 \r\nL 778.050902 68.574048 \r\nL 778.714628 68.293582 \r\nL 780.174826 67.978057 \r\nL 780.174826 67.90794 \r\nL 780.241199 67.90794 \r\nL 786.944834 66.400433 \r\nL 788.670523 66.155025 \r\nL 796.568865 64.542343 \r\nL 796.635238 64.437168 \r\nL 798.560044 63.946352 \r\nL 798.560044 63.876236 \r\nL 799.356516 63.771061 \r\nL 800.285733 63.665886 \r\nL 800.551223 63.525653 \r\nL 800.683968 63.525653 \r\nL 803.670737 63.17507 \r\nL 803.737109 63.034836 \r\nL 804.334463 62.96472 \r\nL 808.316821 62.018146 \r\nL 809.31241 61.457213 \r\nL 809.31241 61.387096 \r\nL 811.170844 61.001455 \r\nL 817.078008 60.019822 \r\nL 817.476244 59.739356 \r\nL 818.206342 59.634181 \r\nL 818.206342 59.599123 \r\nL 820.330267 59.318656 \r\nL 820.728502 59.108307 \r\nL 820.993993 59.003132 \r\nL 820.993993 58.968073 \r\nL 822.387818 58.652549 \r\nL 822.985172 58.61749 \r\nL 822.985172 58.477257 \r\nL 823.44978 58.407141 \r\nL 823.44978 58.337024 \r\nL 823.648898 58.337024 \r\nL 823.648898 58.301966 \r\nL 825.308214 57.916324 \r\nL 826.702039 57.425508 \r\nL 828.75959 57.109983 \r\nL 831.879104 56.7594 \r\nL 835.131363 56.163409 \r\nL 835.662344 56.128351 \r\nL 836.193325 56.023176 \r\nL 837.122542 55.882943 \r\nL 837.786268 55.707651 \r\nL 837.786268 55.602476 \r\nL 838.84823 55.357068 \r\nL 839.246466 55.286952 \r\nL 839.246466 55.216835 \r\nL 839.312838 55.216835 \r\nL 839.379211 55.146719 \r\nL 839.644702 55.146719 \r\nL 844.224413 54.375436 \r\nL 844.556276 54.375436 \r\nL 847.277554 54.024853 \r\nL 847.476672 53.919678 \r\nL 849.003242 53.814503 \r\nL 850.994421 53.498978 \r\nL 850.994421 53.46392 \r\nL 851.923638 53.358745 \r\nL 852.9856 53.148395 \r\nL 853.516581 53.04322 \r\nL 853.649326 53.008162 \r\nL 854.976779 52.832871 \r\nL 855.109524 52.797812 \r\nL 856.702467 52.587463 \r\nL 859.291 52.482288 \r\nL 859.291 52.377113 \r\nL 859.490118 52.377113 \r\nL 864.136202 51.535713 \r\nL 864.667183 51.325364 \r\nL 865.86189 51.150072 \r\nL 865.86189 51.079956 \r\nL 867.985814 50.974781 \r\nL 868.782286 50.659256 \r\nL 872.034544 50.519023 \r\nL 874.556704 50.273615 \r\nL 874.95494 49.95809 \r\nL 875.751412 49.852915 \r\nL 875.751412 49.782798 \r\nL 876.614256 49.677624 \r\nL 876.614256 49.572449 \r\nL 877.875336 49.32704 \r\nL 879.070043 49.221866 \r\nL 879.202788 49.116691 \r\nL 880.065632 49.011516 \r\nL 880.397496 48.906341 \r\nL 882.654165 48.625874 \r\nL 882.720538 48.485641 \r\nL 885.840051 48.380466 \r\nL 885.840051 48.31035 \r\nL 887.963975 47.819534 \r\nL 889.09231 47.714359 \r\nL 889.09231 47.609184 \r\nL 889.158683 47.609184 \r\nL 889.822409 47.293659 \r\nL 892.742805 46.627551 \r\nL 893.273786 46.45226 \r\nL 893.406531 46.382143 \r\nL 893.804767 46.312027 \r\nL 893.804767 46.276968 \r\nL 894.534866 46.136735 \r\nL 895.596828 46.03156 \r\nL 895.729573 45.961444 \r\nL 896.791535 45.856269 \r\nL 896.791535 45.82121 \r\nL 896.857908 45.82121 \r\nL 897.123398 45.786152 \r\nL 897.986242 45.680977 \r\nL 898.052615 45.610861 \r\nL 899.048204 45.120044 \r\nL 900.508402 44.699345 \r\nL 900.70752 44.524053 \r\nL 901.636737 44.418878 \r\nL 901.636737 44.38382 \r\nL 902.698699 44.278645 \r\nL 902.831444 44.208528 \r\nL 903.096935 44.208528 \r\nL 903.096935 44.17347 \r\nL 903.296053 44.033237 \r\nL 903.827034 43.928062 \r\nL 903.827034 43.893004 \r\nL 904.689878 43.822887 \r\nL 904.689878 43.787829 \r\nL 906.349193 43.437246 \r\nL 906.481939 43.437246 \r\nL 908.804981 42.806196 \r\nL 909.53508 42.73608 \r\nL 909.667825 42.595847 \r\nL 911.194395 42.490672 \r\nL 911.194395 42.455613 \r\nL 912.189985 42.350438 \r\nL 912.920084 41.999855 \r\nL 913.849301 41.929739 \r\nL 918.296267 41.368806 \r\nL 919.225483 41.263631 \r\nL 921.216662 41.018223 \r\nL 921.216662 40.983165 \r\nL 923.008723 40.87799 \r\nL 923.075096 40.842932 \r\nL 924.601666 40.597523 \r\nL 925.19902 40.492349 \r\nL 925.265393 40.387174 \r\nL 926.194609 40.317057 \r\nL 926.194609 40.24694 \r\nL 926.592845 40.141765 \r\nL 927.190199 40.106707 \r\nL 927.190199 40.001532 \r\nL 928.119416 39.931416 \r\nL 928.849515 39.721066 \r\nL 934.424815 38.984842 \r\nL 941.194824 38.564142 \r\nL 942.920512 38.458967 \r\nL 942.920512 38.423909 \r\nL 947.301105 38.038267 \r\nL 948.761303 37.722743 \r\nL 949.624147 37.617568 \r\nL 949.624147 37.582509 \r\nL 950.619737 37.477335 \r\nL 950.885227 37.442276 \r\nL 952.279053 37.337101 \r\nL 961.571221 36.635935 \r\nL 961.637593 36.600877 \r\nL 963.82789 36.53076 \r\nL 965.221715 36.145119 \r\nL 973.451921 35.373836 \r\nL 975.17761 35.268662 \r\nL 975.243982 35.233603 \r\nL 976.372317 35.163487 \r\nL 980.952028 34.988195 \r\nL 981.7485 34.918079 \r\nL 982.876834 34.812904 \r\nL 993.164592 34.67267 \r\nL 993.164592 34.67267 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path d=\"M 508.949716 294.118125 \r\nL 508.949716 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_9\">\r\n    <path d=\"M 1016.222443 294.118125 \r\nL 1016.222443 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_10\">\r\n    <path d=\"M 508.949716 294.118125 \r\nL 1016.222443 294.118125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_11\">\r\n    <path d=\"M 508.949716 22.318125 \r\nL 1016.222443 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_26\">\r\n    <!-- ROC Curve -->\r\n    <defs>\r\n     <path d=\"M 39.40625 66.21875 \r\nQ 28.65625 66.21875 22.328125 58.203125 \r\nQ 16.015625 50.203125 16.015625 36.375 \r\nQ 16.015625 22.609375 22.328125 14.59375 \r\nQ 28.65625 6.59375 39.40625 6.59375 \r\nQ 50.140625 6.59375 56.421875 14.59375 \r\nQ 62.703125 22.609375 62.703125 36.375 \r\nQ 62.703125 50.203125 56.421875 58.203125 \r\nQ 50.140625 66.21875 39.40625 66.21875 \r\nz\r\nM 39.40625 74.21875 \r\nQ 54.734375 74.21875 63.90625 63.9375 \r\nQ 73.09375 53.65625 73.09375 36.375 \r\nQ 73.09375 19.140625 63.90625 8.859375 \r\nQ 54.734375 -1.421875 39.40625 -1.421875 \r\nQ 24.03125 -1.421875 14.8125 8.828125 \r\nQ 5.609375 19.09375 5.609375 36.375 \r\nQ 5.609375 53.65625 14.8125 63.9375 \r\nQ 24.03125 74.21875 39.40625 74.21875 \r\nz\r\n\" id=\"DejaVuSans-79\"/>\r\n    </defs>\r\n    <g transform=\"translate(729.895455 16.318125)scale(0.12 -0.12)\">\r\n     <use xlink:href=\"#DejaVuSans-82\"/>\r\n     <use x=\"69.482422\" xlink:href=\"#DejaVuSans-79\"/>\r\n     <use x=\"148.193359\" xlink:href=\"#DejaVuSans-67\"/>\r\n     <use x=\"218.017578\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"249.804688\" xlink:href=\"#DejaVuSans-67\"/>\r\n     <use x=\"319.628906\" xlink:href=\"#DejaVuSans-117\"/>\r\n     <use x=\"383.007812\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"424.121094\" xlink:href=\"#DejaVuSans-118\"/>\r\n     <use x=\"483.300781\" xlink:href=\"#DejaVuSans-101\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_12\">\r\n     <path d=\"M 868.966193 289.118125 \r\nL 1009.222443 289.118125 \r\nQ 1011.222443 289.118125 1011.222443 287.118125 \r\nL 1011.222443 273.44 \r\nQ 1011.222443 271.44 1009.222443 271.44 \r\nL 868.966193 271.44 \r\nQ 866.966193 271.44 866.966193 273.44 \r\nL 866.966193 287.118125 \r\nQ 866.966193 289.118125 868.966193 289.118125 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\">\r\n     <path d=\"M 870.966193 279.538437 \r\nL 890.966193 279.538437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_19\"/>\r\n    <g id=\"text_27\">\r\n     <!-- Pipeline (AUC = 0.77) -->\r\n     <defs>\r\n      <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n      <path d=\"M 31 75.875 \r\nQ 24.46875 64.65625 21.28125 53.65625 \r\nQ 18.109375 42.671875 18.109375 31.390625 \r\nQ 18.109375 20.125 21.3125 9.0625 \r\nQ 24.515625 -2 31 -13.1875 \r\nL 23.1875 -13.1875 \r\nQ 15.875 -1.703125 12.234375 9.375 \r\nQ 8.59375 20.453125 8.59375 31.390625 \r\nQ 8.59375 42.28125 12.203125 53.3125 \r\nQ 15.828125 64.359375 23.1875 75.875 \r\nz\r\n\" id=\"DejaVuSans-40\"/>\r\n      <path d=\"M 34.1875 63.1875 \r\nL 20.796875 26.90625 \r\nL 47.609375 26.90625 \r\nz\r\nM 28.609375 72.90625 \r\nL 39.796875 72.90625 \r\nL 67.578125 0 \r\nL 57.328125 0 \r\nL 50.6875 18.703125 \r\nL 17.828125 18.703125 \r\nL 11.1875 0 \r\nL 0.78125 0 \r\nz\r\n\" id=\"DejaVuSans-65\"/>\r\n      <path d=\"M 8.6875 72.90625 \r\nL 18.609375 72.90625 \r\nL 18.609375 28.609375 \r\nQ 18.609375 16.890625 22.84375 11.734375 \r\nQ 27.09375 6.59375 36.625 6.59375 \r\nQ 46.09375 6.59375 50.34375 11.734375 \r\nQ 54.59375 16.890625 54.59375 28.609375 \r\nL 54.59375 72.90625 \r\nL 64.5 72.90625 \r\nL 64.5 27.390625 \r\nQ 64.5 13.140625 57.4375 5.859375 \r\nQ 50.390625 -1.421875 36.625 -1.421875 \r\nQ 22.796875 -1.421875 15.734375 5.859375 \r\nQ 8.6875 13.140625 8.6875 27.390625 \r\nz\r\n\" id=\"DejaVuSans-85\"/>\r\n      <path d=\"M 10.59375 45.40625 \r\nL 73.1875 45.40625 \r\nL 73.1875 37.203125 \r\nL 10.59375 37.203125 \r\nz\r\nM 10.59375 25.484375 \r\nL 73.1875 25.484375 \r\nL 73.1875 17.1875 \r\nL 10.59375 17.1875 \r\nz\r\n\" id=\"DejaVuSans-61\"/>\r\n      <path d=\"M 8.015625 75.875 \r\nL 15.828125 75.875 \r\nQ 23.140625 64.359375 26.78125 53.3125 \r\nQ 30.421875 42.28125 30.421875 31.390625 \r\nQ 30.421875 20.453125 26.78125 9.375 \r\nQ 23.140625 -1.703125 15.828125 -13.1875 \r\nL 8.015625 -13.1875 \r\nQ 14.5 -2 17.703125 9.0625 \r\nQ 20.90625 20.125 20.90625 31.390625 \r\nQ 20.90625 42.671875 17.703125 53.65625 \r\nQ 14.5 64.65625 8.015625 75.875 \r\nz\r\n\" id=\"DejaVuSans-41\"/>\r\n     </defs>\r\n     <g transform=\"translate(898.966193 283.038437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-80\"/>\r\n      <use x=\"58.052734\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"85.835938\" xlink:href=\"#DejaVuSans-112\"/>\r\n      <use x=\"149.3125\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"210.835938\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"238.619141\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"266.402344\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"329.78125\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"391.304688\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"423.091797\" xlink:href=\"#DejaVuSans-40\"/>\r\n      <use x=\"462.105469\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"530.513672\" xlink:href=\"#DejaVuSans-85\"/>\r\n      <use x=\"603.707031\" xlink:href=\"#DejaVuSans-67\"/>\r\n      <use x=\"673.53125\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"705.318359\" xlink:href=\"#DejaVuSans-61\"/>\r\n      <use x=\"789.107422\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"820.894531\" xlink:href=\"#DejaVuSans-48\"/>\r\n      <use x=\"884.517578\" xlink:href=\"#DejaVuSans-46\"/>\r\n      <use x=\"916.304688\" xlink:href=\"#DejaVuSans-55\"/>\r\n      <use x=\"979.927734\" xlink:href=\"#DejaVuSans-55\"/>\r\n      <use x=\"1043.550781\" xlink:href=\"#DejaVuSans-41\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n  <g id=\"axes_3\">\r\n   <g id=\"patch_13\">\r\n    <path clip-path=\"url(#p98ec3d0cce)\" d=\"M 331.404261 294.118125 \r\nL 331.404261 293.056406 \r\nL 331.404261 23.379844 \r\nL 331.404261 22.318125 \r\nL 344.994261 22.318125 \r\nL 344.994261 23.379844 \r\nL 344.994261 293.056406 \r\nL 344.994261 294.118125 \r\nz\r\n\" style=\"fill:#ffffff;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.01;\"/>\r\n   </g>\r\n   <image height=\"271\" id=\"imagef8c3b25895\" transform=\"scale(1 -1)translate(0 -271)\" width=\"14\" x=\"331\" xlink:href=\"data:image/png;base64,\r\niVBORw0KGgoAAAANSUhEUgAAAA4AAAEPCAYAAAByY/XAAAAABHNCSVQICAgIfAhkiAAAAXZJREFUaIHtm8sNwzAMQ2Uno3WG7j9J0wmSwyNKgbVzJyjxYzdAOl7zfRV4zjEHwdVZYzKgn7E4IwOyOavqHJPuCEc9q4ERAkdMAKLsaNjRPypTlRc5KXI5Pl58VISLsmMFceiOSpEp0O8jwymjNojjZsSq+vvoT47dDl5kYVQKpPejf1R/rfaOj4wQKBzIlBEChRs5yI7N+BPGoGvODcwJgHICQOD28YmR4RYRZyfn9lmiyDsA90/US1mMqkk+1mC3xw7Aj4BmO7KuAChOh48M2FDkFXyE751ZkSu/qiswMmBDkbE4uB1BAYCEkqpuO2bDqP+/44TnqvAtWZA4OYzUx45Rg4BU1ZwdeZGjVP24GXN2PBp2dNtxBI3qL3L57fCrShlpcjgjLrI95B2MMVmlAVCK7G+H+9IRAuCOnHCxmiMnqGqPHD9zcoosnKv2U65hVP+PQISTskoZ+ajw6zVhVMwIgcKO7s/CBXHgP/W4qvT/j3TUL7WjjR5hEDXdAAAAAElFTkSuQmCC\" y=\"-22\"/>\r\n   <g id=\"matplotlib.axis_5\"/>\r\n   <g id=\"matplotlib.axis_6\">\r\n    <g id=\"ytick_9\">\r\n     <g id=\"line2d_20\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 3.5 0 \r\n\" id=\"mf3513137dd\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"344.994261\" xlink:href=\"#mf3513137dd\" y=\"279.002892\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_28\">\r\n      <!-- 0.3 -->\r\n      <g transform=\"translate(351.994261 282.80211)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_10\">\r\n     <g id=\"line2d_21\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"344.994261\" xlink:href=\"#mf3513137dd\" y=\"218.610508\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_29\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(351.994261 222.409727)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_11\">\r\n     <g id=\"line2d_22\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"344.994261\" xlink:href=\"#mf3513137dd\" y=\"158.218125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_30\">\r\n      <!-- 0.5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(351.994261 162.017344)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_12\">\r\n     <g id=\"line2d_23\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"344.994261\" xlink:href=\"#mf3513137dd\" y=\"97.825742\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_31\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(351.994261 101.62496)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_13\">\r\n     <g id=\"line2d_24\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"344.994261\" xlink:href=\"#mf3513137dd\" y=\"37.433358\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_32\">\r\n      <!-- 0.7 -->\r\n      <g transform=\"translate(351.994261 41.232577)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_14\">\r\n    <path d=\"M 331.404261 294.118125 \r\nL 331.404261 293.056406 \r\nL 331.404261 23.379844 \r\nL 331.404261 22.318125 \r\nL 344.994261 22.318125 \r\nL 344.994261 23.379844 \r\nL 344.994261 293.056406 \r\nL 344.994261 294.118125 \r\nz\r\n\" style=\"fill:none;stroke:#000000;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pf90ac678d3\">\r\n   <rect height=\"271.8\" width=\"271.8\" x=\"34.240625\" y=\"22.318125\"/>\r\n  </clipPath>\r\n  <clipPath id=\"pfcf3296556\">\r\n   <rect height=\"271.8\" width=\"507.272727\" x=\"508.949716\" y=\"22.318125\"/>\r\n  </clipPath>\r\n  <clipPath id=\"p98ec3d0cce\">\r\n   <rect height=\"271.8\" width=\"13.59\" x=\"331.404261\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "y_pred = fit.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "fig = plt.figure(1, figsize=(20, 5))\n",
    "\n",
    "chart_1 = fig.add_subplot(121)\n",
    "chart_2 = fig.add_subplot(122)\n",
    "\n",
    "# Pass Fitted Model, and our test sets, see how they do\n",
    "plot_confusion_matrix(pipe1, X_test, y_test, normalize='true', ax=chart_1)\n",
    "chart_1.set_title('Confusion Matrix')\n",
    "\n",
    "plot_roc_curve(pipe1, X_test, y_test, ax=chart_2)\n",
    "chart_2.set_title('ROC Curve')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 64-bit ('ML7331': conda)",
   "language": "python",
   "name": "python37164bitml7331condacf2794d0fef349cfa281e1878aa68ff6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}