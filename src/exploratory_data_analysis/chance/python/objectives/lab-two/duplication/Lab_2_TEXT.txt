Data Preparation Part 1

The dataset used in this notebook consists of two datasets, a cardiovascular dataset and a smoking dataset. Each dataset is derived from the same data. This data was uploaded to the Kaggle website by Svetlana Ulianova, who appears to be a data science student at Ryerson University. Ms. Ulianova did not provide a source nor an exact purpose of the dataset. Further, Ms. Ulianova did not sponsor a competition or add a description with the upload of this dataset. The data provided on Kaggle includes 70,000 rows and 12 different features, including whether a patient has or does not have cardiovascular disease and if a patient is a smoker or a non-smoker. With these two final datasets, there is one containing cardiovascular disease and another containing a smoking dataset. Therefore, the modeling problem for each of the cardiovascular disease and smoking datasets is defined as a classification problem. An effective outcome may be measured using the receiver operating characteristic and recall metrics, discussed below in more detail.

Important note, the original intention of the dataset was to have CVD as the target variable. We are additionally using the dataset below to treat Smoking as a target variable, since the assignment requires two classification tasks. Therefore, this notebook develops models to predict whether a person has cardiovascular disease and whether a person is a smoker based on the available data. Therefore, the class variables are “Cardiovascular Disease” and “Smoking.” Both class variables “Cardiovascular Disease” and “Smoking” are binary, categorical variables. Further variable representations can be seen in Table 1.

As can be seen in Table 1, the datasets used for classifying whether a person has cardiovascular disease, and whether a person is a smoker, use 11 features and 1 target variable. That target variable being whether an observation has cardiovascular disease for the cardiovascular disease dataset and is a smoker for the smoking dataset, respectively.

The 11 features consist of three different types of input features. These include objective features which contain factual information, examination features which contain medical examination information, and subjective features which contain patient response information. Further, the datasets each include 70,000 rows used in each of the cardiovascular disease dataset and the smoking dataset. To examine and provide classification of the presence of cardiovascular disease and whether a person is a smoker, relationships have been evaluated between each of the outlined features. This has been conducted by completing all available observations (or rows) through imputation, scaling, or one-hot encoding, values using a median or mean value for those that are missing.

Certain pre-processing techniques are used to prepare both the cardiovascular disease and smoking datasets. These pre-processing methods use a variety of libraries used for data analysis in Python. The libraries used for pre-processing include the “RobustScaler,” “StandardScaler,” “OneHotEncoder,” “SimpleImputer,” and “ColumnTransformer” functions within the scikit-learn package. Specifically, the “RobustScaler,” “StandardScaler,” and “OneHotEncoder” functions are used in the “preprocessing” component of the scikit-learn package while the “SimpleImputer,” and “ColumnTransformer” are used in the “impute” and “compose” components of the scikit-learn package, respectively. The RobustScaler() function from the scikit-learn python package is used to standardize values between features. The RobustScaler() function scales features using statistical analysis that is robust to outliers in a dataset. (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html). A variety of different parameters may be adjusted to improve performance of each of the models. For example, instead of using the RobustScaler() function, the StandardScalar() function may be implemented from the scikit-learn python package. The StandardScalar() function removes the mean and scales to a unit variance to standardize the data.(https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). While not explicitly included in the preprocessing cell, these functions are implemented in a pipeline in later cells.

A one-hot encoding (OHE) has been implemented to standardize attributes such as gender and cholesterol. However, since these categorical attributes were recorded as numerical variables when recorded into the dataset, OHE is not expected to impact the performance of the classifiers. For example, the value of gender was recorded as 1 or 2 instead of 0 and 1, which is still indicative of an ordinal relationship. Therefore, OHE may aid in standardizing the data, but in this instance, not impact performance of the models because of the encoding that occurred when the data were recorded. Stated sufficiently, OHE may be redundant.

Initially, the data is loaded into a Python dataframe named “df,” which is then copied into another Python dataframe “df_clean”, where duplicates are dropped. Next, the variable “age,” which is an integer in days, is converted into years while maintaining the type “int” for the “age” variable. The categorical variable “gender” was originally encoded as being “1” and “2.” For consistency and clarity of interpretation, the “gender” variable was reencoded to define the input “male” as “1” and “female” as “0”.

Feature engineering is also performed using the “height” and “weight” variables. The variable “height” is an integer while the variable “weight” is a float. Generally, the height and weight of an individual are related through the “Body Mass Index” (BMI). Extrapolating further, the weight divided by the squared quantity of height divided by 100 provides an indication of the BMI of a person. Both the cardiovascular disease and smoking datasets use the “height” and “weight” variables provided to calculate and use BMI as a variable in the cardiovascular disease and smoking datasets. After the BMI is calculated for each dataset, the BMI is grouped based on resulting numerical, BMI values. The BMI values are separated into 4 categories based on the BMI value calculated. A new variable “bmiGrp” is used to separate the 4 new categories of BMI. Specifically, a BMI value being less than 18.5 is represented as “1” in the “bmiGrp” variable, a BMI value being between 18.5 and 25 is represented as “2” in the “bmiGrp” variable, a BMI value between 25 and 30 is represented as “3” in the “bmiGrp” variable, and a BMI value being greater than 30 is represented as “4” in the “bmiGrp” variable. Binning the BMI variable may provide greater indication of whether a person has cardiovascular disease or is a smoker by allowing for an analysis of potential separation of the groups. For example, through comparing those who have cardiovascular disease and those who are smokers across groups. Prediction accuracy my improve based on the separation between groups.

Similarly, the variables “ap_hi” and “ap_lo” represent systolic and diastolic blood pressure, respectively. Both variables are represented as integers. The variables are combined and binned into a single variable representing blood pressure. Specifically, the “ap_hi” and “ap-lo” variables are combined into a new variable labeled “bp” and binned into 5 distinct categories. Specifically, the “bp” variable includes a category represented by “1” that divides blood pressure such that the “ap_hi” variable is less than 120 and the “ap_lo” variable is less than 80, a category represented by “2” that divides blood pressure such that the “ap_hi” variable is between 120 and 130 and the “ap_lo” variable is less than 80, a category represented by “3” that divides blood pressure such that the “ap_hi” variable is between 130 and 140 and the “ap_lo” variable is between 80 and 90, a category represented by “4” that divides blood pressure such that the “ap_hi” variable is greater than or equal to 140 or the “ap_lo” variable greater than or equal to 90, and a category represented by “5” that divides blood pressure such that the “ap_hi” variable is greater than 180 or the “ap_lo” variable is greater than 120.

Data Preparation Part 2

As mentioned above, the final dataset used in this notebook consits of two different datasets. With the variables created and categorized, the final dataset used for classifying the presence of cardiovascular disease includes columns “age,” “gender,” “bmiGrp,” “bp,” “cholesterol,” “gluc,” “smoke,” “alco,” and “active.” Similarly, the final dataset used for classifying whether a person is a smoker includes columns “age,” “gender,” “bmiGrp,” “bp,” “cholesterol,” “gluc,” “cardio,” “alco,” and “active.” The cardiovascular disease dataset has 9 attributes that are used to classify the response variable of “cardio” indicative of having cardiovascular disease. The smoking dataset also has 9 attributes that are used to classify the response variable of “smoke” indicative of whether a person is a smoker. Both the cardiovascular disease and smoking datasets 69,776 rows after cleaning the datasets.

As discussed above, variables “bp” and “bmiGrp” are newly formed variables used in the analysis of the cardiovascular disease and smoking datasets. The “bp” variable combines the “ap_hi” and “ap_lo” variables into a single category based on individual values in each of those attributes. This allows for an analysis of whether a combined blood pressure may be indicative of cardiovascular disease by examining and comparing the categories. Similarly, an analysis is used to analyze whether a combined blood pressure may be indicative of smoking. The newly formed “bmiGrp” is based on a calculated BMI value from the listed height and weight of each row that was recorded for each of the cardiovascular disease and smoking datasets. Again, the “bmiGrp” bins rows into categories based on the calculated values. This also allows for an analysis of whether a BMI above a threshold is indicative of cardiovascular disease. The “bmiGrp” variable also provides an analysis of whether BMI being above a threshold is indicative of a person being a smoker. Each of the “bmiGrp” and “bp” variables use all of the rows in each of the cardiovascular disease and smoking datasets.

Modeling and Evaluation 1

As the response variable, cardiovascular disease, is balanced, an effective outcome may be measured by using the receiver operating characteristic metric. The cardiovascular disease dataset uses the receiver operating characteristic (“ROC”) metric to evaluate performance of the classification models. The ROC metric characterizes a trade-off between true and false positives in a classification model. Here, the ROC metric is used as a threshold related to the accuracy of the classification model used for cardiovascular disease. The ROC graph compares a curve of false positives by true positives against a straight, diagonal line indicative of random guessing. The area under the curve compared against the diagonal line is indicative of performance of the classification model. For example, a model having greater area under the curve closer to the origin provides better classification accuracy for small false-positive rates, whereas a model having greater area under the curve further away from the origin, closer to 1, provides better classification accuracy for larger false-positive rates. The false-positive rate is equal to the false-positive values divided by the total number of negative values. Likewise, the true-positive rate is equal to the true-positive values divided by the total number of positive values.

Based on this description of the ROC metric and the analysis of the cardiovascular dataset, it is appropriate to analyze the classification models outlined in this notebook. This can be explained further as the response variable, cardiovascular disease, has a high number of values being indicative of a lot of people having cardiovascular disease, therefore, the ROC metric provides a robust metric for evaluating performance of models using the false-positive rates of each of the models. The ROC metric provides both metrics and visuals which are easily interpretable enabling the evaluation of the performance of classification models. Further, in the instance of predicting the presence of cardiovascular disease, the false-positive rate is an important metric to evaluate. For example, treating cardiovascular disease when no cardiovascular disease is present (which would be a false-positive in this instance) is harmful to an otherwise healthy patient. This is to say that a patient whom undergoes a surgical intervention without having cardiovascular disease is subjected to significant and unnecessary trauma. In the absence of a surgical intervention, a healthy patient taking medication unnecessarily still provides harm to the patient. Therefore, when evaluating models designed to predict the presence of cardiovascular disease, the ROC metric and curve provide a method to analyze which models have the lowest false-positive rates when predicting the presence of cardiovascular disease.

The response variable in the smoking dataset is whether a person is a smoker, which is denoted in the dataset as the attribute “smoke.” The analysis provides a prediction of whether a person is or is not smoker, which is indicated by a column in the smoking dataset titled “smoke.” The “smoke” column of the smoking dataset is a binary, categorical variable. Therefore, the modeling problem for the smoking dataset is defined as a classification problem. Since the smoking dataset is not balanced, an effective outcome may be measured using the recall metric, discussed below in more detail.

The smoking dataset uses the recall metric to evaluate performance of the classification models. Again, the response variable, being whether a person smokes, is imbalanced. This implies that there are very few smokers included in the dataset. The lack of representation of smokers in this dataset makes the ROC curve and metric unsuitable for model performance evaluation. The recall metric provides an indication of the false negatives in the classification performance of the model. From this measure of false negatives, an analysis can be produced. It is important to note that recall is also a cost-sensitive measure. Specifically, recall is equal to the total true-positive predictions divided by the total quantity of the true-positive predictions added to the total false-negative values from the predictions of the classification models. When evaluating the recall metric, the higher the recall value, less false-negative values are present in the prediction.

Based on the discussion above, it is appropriate to analyze the classification models surrounding the smoking dataset with the recall metric. The response variable being smoking has a low number of values being indicative of very few people being smokers, and, therefore, the recall metric provides a more robust metric for evaluating performance of a model using the false-negative rates of each of the models. The recall metric provides an easily interpretable metric to evaluate the performance of the classification models such that the higher recall value, the lower number of false negatives are being predicted. Further, in the instance of predicting whether a person is a smoker, the false-negative rate is an important metric to evaluate. For example, when determining whether a person is a smoker, such as during a health insurance premium determination; reducing the false-negative values that is smokers predicted as not smokers is valuable. It is generally accepted that a person who smokes presents a higher risk for needing long-term hospital care based on the damage to the lungs from the smoking. Being at higher risk for expensive future medical bills, a health insurance company may charge higher premiums to offset this voluntary risk. Predicting someone as not a smoker when they do smoke, a false-negative, increases the risk that the health insurance company will need to pay for expensive medical bills without having recovered some of the losses with higher premiums. Therefore, when evaluating models designed to predict whether a person is a smoker, the recall metric provides a method to analyze which models have the lowest false-negative rates when predicting if a person is a smoker.

The smoking dataset response variable allows for the evaluation by use of the ROC metric. The ROC metric in the smoking dataset measures the false-positive rate of the response variable, or in this instance, a measure of the predictions of those who do not smoke as those who do smoke. In the example provided, this would cause a person who is not a smoker to incur costs of someone who does smoke, which causes unnecessary expenses on those people labeled smokers who are not smokers. Therefore, in order to reduce this type of error, the ROC metric is appropriate to evaluate models run on the smoking dataset, after the smoking dataset is up sampled, as described.

Modeling and Evaluation 2

The response variable in the cardiovascular dataset is whether a person has cardiovascular disease, which is denoted in the dataset as the attribute “cardio.” The analysis provides a prediction of whether a person has the presence or absence of cardiovascular disease, which is indicated by a column in the cardiovascular disease dataset titled “cardio.” The “cardio” column of the cardiovascular disease dataset is a binary, categorical variable. Therefore, the modeling problem for the cardiovascular disease dataset is defined as a classification problem. Since the response variable, cardiovascular disease, is balanced, stratified cross-validation is used to divide the cardiovascular disease dataset.

The method used in this notebook to divide the data into training and testing splits is 10-fold, stratified cross-validation with an 80/20 training/testing split. Generally, cross-validation partitions data into a number of disjoint subsets. The number of folds, in this instance 10, represents the number of those disjoint subsets. For example, a 10-fold cross validation trains on (10-1), or 9, partitions with testing being done the remaining partition. Training the classifier using 9 partitions allows for separation between the testing and training data such that the classifier does not train on the testing data. Keeping the testing data separate from the training data ensures that the metrics used to evaluate each of the classifiers for the cardiovascular disease dataset provide an accurate indication of performance of the model. This is why the cardiovascular disease dataset uses an 80/20 split, being indicative of 80% of the data being used for training and 20% of the data being separated for testing. As stated, however, the cardiovascular disease dataset uses stratified cross validation to split the testing and training data.

Stratified cross validation uses two different classes having the same frequency in each fold of the cross validation. This typically occurs after 10 folds in stratified cross validation, and thus the number of folds used with the cardiovascular disease dataset is set to 10. Therefore, the method used to divide the cardiovascular disease dataset is a 10-fold, stratified cross validation.

The 10-fold, stratified cross validation is appropriate for the cardiovascular disease dataset because the 10-fold, stratified cross validation ensures an equal amount of positive and negative classes that occur with the same frequency. Since, the important metric for analysis with the cardiovascular disease dataset is the false-positive rate, having an equal distribution of positive and negative classes ensures that the false-positive rate (as evaluated by the ROC metric and curve) is not skewed. For example, when analyzing a number of observations, a certain number of false positives may appear to have a higher or lower rate due to the imbalance in the data. Since the 10-fold, stratified cross validation ensures the same frequency between positive and negative classes, the false-positive rate is normalized providing more accurate results.

Since the smoking dataset is not balanced, an effective method for dividing the smoking dataset is the 10-fold, stratified cross validation using an 80/20 training/testing split of the smoking dataset. As described in explicit detail above, the 10-fold stratified cross validation method provides a balance between positive and negative classes. Here, the response variable of whether a person is a smoker, has an imbalance in the data meaning the smoking dataset includes very few smokers. This skews the data and as such, the recall metric being used to evaluate the models since a few false-negative values that occur with an imbalanced dataset will generally skew the rate of occurrence of the false-negative values found in the model. Therefore, to account for this imbalance on the response variable in the smoking dataset, the 10-fold, stratified cross validation is used to eliminate or reduce skew caused by an unequal amount of positive and negative classes. The imbalance in the response variable of the smoking dataset, being whether a person is a smoker, makes using the 10-fold, stratified cross validation the most appropriate method for dividing the data.

The smoking dataset may be up-sampled to account for the imbalance of the categorical response variable of whether a person is or is not a smoker. Again, the Python package “imbalanced-learn” can be used to up-sample the smoking dataset to correct for the imbalance in the smoking response variable. Using the “imbalanced-learn” Python package makes the 10-fold, stratified cross validation less important based on the dataset achieving balance with the up sample. However, the 10-fold, stratified cross validation is still the most appropriate method to use to divide the smoking dataset.

Therefore, both the cardiovascular disease and smoking datasets use a 10-fold, stratified cross validation method to separate the data into training and testing partitions. The 10-fold stratified cross validation is the most appropriate method to divide data from both the cardiovascular disease and smoking datasets into training and testing splits.

Modeling and Evaluation 3

For the cardiovascular disease dataset, the classification models in this notebook include a Naïve-Bayes, k-NN, and logistic regression algorithm. At first, all of the data was utilized and the attributes are split into numerical and categorical features. This allows for the pre-processing discussed above, such as imputing a median value, to be done through pipelines, providing an efficient technique for processing data. Using these pipelines, the cardiovascular disease dataset is passed through to the classification algorithms, in this case Naïve-Bayes, k-NN, and logistic regression algorithms. Again, performance of these algorithms is evaluated against the ROC metric to understand the false-positive rate of each of the algorithms. Again, the closer the value to 1, the ROC metric and curve measure more area under the curve. Applying this metric to each of the classification models determines which of the models have the greatest accuracy when predicting the presence of cardiovascular disease.

After running the 10-fold, stratified cross validation, each of the classification models calculated a ROC value,shown below. the ROC for the Naïve-Bayes model is 0.703, the ROC for the k-NN model is 0.777, and the ROC for the logistic regression model is 0.768. Therefore, in comparison, the k-NN model provides the greatest ROC value between the competing models and provides the greatest indication of predicting cardiovascular disease. This suggests that the most appropriate model to predict cardiovascular disease can have arbitrary decision boundaries, in which a distance metric is used to compute distance between records. The next best performing model is the logistic regression model. The ROC from the logistic regression only slightly less than the ROC for the k-NN model, while the Naïve-Bayes model is much less than both the k-NN and logistic regression models. This suggests that, while the ROC for the k-NN and logistic regression are competitive, the Naïve-Bayes model is not the most appropriate model to predict the presence of cardiovascular disease in a patient.

Also, while the k-NN and logistic regression models have ROC values greater than 0.75, in the domain of disease, and specifically cardiovascular disease, a much higher ROC value is needed. For example, with a ROC value at 0.777 and 0.768 for the k-NN and logistic regression models still include a lot of error that may cause harm to a person who is predicted as having cardiovascular disease but does not. Therefore, while these models show improved performance, the k-NN and logistic regression models may not provide valuable predictions based on the ROC values. In a real world example, predicting any person as having cardiovascular disease but does not provides the potential for that person to incur harm, and thus, would likely need improvement before implementation and deployment. Parameter adjustments are just one of many things that could be considered.

For the smoking dataset, the classification models include a random forest, decision tree, and gradient boosting algorithm. Initially, the smoking dataset is up sampled using the “SMOTE” library. This allows for further model performance improvement and or increased accuracy. The attributes are then split into numerical and categorical features. Using the pipelines in this notebook, the smoking dataset is passed through to the aforementioned classification algorithms. It is important to remember that the performance of these algorithms is evaluated against the recall metric. This allows us to understand the false-negative rate of each of the algorithms. Again, the higher the recall value, a lower number of false negatives are present in the prediction. Examining this metric, it is possible to evaluate each of the classification models and determine which of the models have the greatest accuracy when predicting whether someone is a smoker or not.

After running the 10-fold, stratified cross validation, each of the classification models calculates a recall value. Shown below, the recall for the random forest algorithm is 0.673, the recall for the decision tree algorithm is 0.830, and the recall for the gradient boosting algorithm is 0.868. Therefore, in comparison, the gradient boosting model provides the greatest recall value between the competing models and provides the greatest indication of accurately predicting whether a person is a smoker. This suggests that the most appropriate model to predict whether a person is a smoker is an iterative procedure that updates weights of not well classified instances. The model which performed most closely was the decision tree model. The recall from the decision tree model is only slightly less than the recall for the gradient boosting model, while the random forest model is only slightly less than the decision tree model. This implies that while the recall values suggest that the gradient boosting model is the most appropriate, all of the models (random forest, decision tree, and gradient boosting) are competitive. For a prediction of whether a person is a smoker, each of these recall values may have secondary effects, which may impact deployment, such as computation time. Since the harm to a person to be wrongly classified as a smoker is one of inconvenience, for example needing to submit a physical, additional paperwork, or higher fees, and can be easily corrected, other factors may dictate any one model's deployment.

Also, the random forest, decision tree, and gradient boosting models all have recall values less than 0.88. This low recall for each of the competing models suggest that many false positives are present in the prediction. This does not provide valuable predictions to identify a person who may require higher health care costs by being a smoker, and thus subjected to higher health insurance premiums. For example, with a recall value at 0.868 and 0.673, the gradient boosting and random forest models, respectively, do not perform well in accurately predicting a person as a smoker with minimizing the predictions of non-smokers as smokers. Therefore, while these models show improved performance, the random forest, gradient boosting, and decision tree models may not provide valuable predictions based on the recall values. Parameter adjustments may once again also aid to further improve performance.

To better understand and evaluate the smoking dataset, a comparison was performed between SMOTEing the entire smoking data set, and not. This example was done using a Decision Tree classifier as an example. The purpose is to show that recall is affected when SMOTE impacts test data. SMOTE in fact should only be applied to the training data. When using pipeline with imblearn, SMOTE will only be applied to the traning folds, and not the validation folds. See the following references for more information. If SMOTE is applied to the whole set, the recall value was 0.89, and 0.34 when SMOTE isn't applied. 0.34 is a more realistic number.

https://stackoverflow.com/questions/50245684/using-smote-with-gridsearchcv-in-scikit-learn https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/

The parameters of each of the models for both of the cardiovascular disease classification problem and the smoking classification problem may be tuned or adjusted to increase generalization performance for both ROC in the cardiovascular disease dataset and recall in the smoking dataset. Individual parameters for each of the models may be tuned to improve performance. For example, in the cell below this notebook executes the “RandomizedSearchCV()” function from the “scikit-learn” Python package to tune model parameters to improve performance of each of the models. The “RandomizedSearchCV()” function compares a fixed number of parameter combinations to achieve the best performing model. Specifically, the “RandomizedSearchCV()” function implements a random search on hyper parameters using “fit” and “score” methodology to tune and generate the best possible model parameters from the randomly searched hyper parameters. The “RandomizedSearchCV()” differs from the “GridSearchCV()” function in that the “RandomizedSearchCV()” function sets as “cv” being the fixed number of parameter combinations, or folds. (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html). This allows the “RandomizedSearchCV()” function to more efficiently process the code. This analysis uses the “RandomizedSearchCV()” function based on this efficiency.

To improve performance of the 3 classification models for each of the cardiovascular disease dataset and the smoking dataset, the “RandomizedSearchCV()” function is used to find optimal hyper parameters for each of the 3 classification models for the cardiovascular disease dataset and the smoking dataset. Specifically, the “RandomizedSearchCV()” function is used setting a “cv” value to 10 to randomly cross validate 10 random parameter combinations to find the most optimal. Further, the “RandomizedSearchCV()” function has the estimator set such that the “RandomizedSearchCV()” function automatically considers and analyzes each of the 3 classification models for each of the cardiovascular disease and smoking datasets. In addition, consistent with the previous analysis, the “RandomizedSearchCV()” function is set to include ROC and recall values using the “scoring” component of the “RandomizedSearchCV()” function for each of the cardiovascular disease and smoking datasets, respectively.

Evaluating further on cardiovascular disease dataset, at first using all of the data, the attributes are split into numerical and categorical features. This allows for the data to undergo pre-processing through the pipelines discussed above. The data is then handed off to the “RandomizedSearchCV()” function to find the optimal performance of each of the models. The “RandomizedSearchCV()” function aids to identify which of the parameters were adjusted, and which are the most important to each of these models.

After running the “RandomizedSearchCV()” function, each of the classification models calculated a ROC value. Shown in the below cell, the ROC for the Naïve-Bayes model is still 0.703, the ROC for the k-NN model improved slightly to 0.774, and the ROC for the logistic regression model improved slightly to 0.769. Thus, when comparing the models the k-NN model provides the best ROC value amongst the other competing models and therefore provides the best indication of predicting cardiovascular disease. The k-NN model provides more valuable information, such as having less false negatives, than the other models that may be considered for deployment. The next closest performing model is the logistic regression model. The ROC from the logistic regression model is still only slightly less than the ROC for the k-NN model, while the Naïve-Bayes model is much less than both the k-NN and logistic regression models. Despite the “RandomizedSearchCV()” function, the performance of the k-NN model and the logistic regression model did not significantly improve, and thus implementation of the “RandomizedSearchCV()” function may be considered too computationally time consuming. In other words, implementing the “RandomizedSearchCV()” function did not significantly improve the results of each of the models, and was computationally time-consuming, which reduces the value of implementing the “RandomizedSearchCV()” function.

While the “RandomizedSearchCV()” function improved model performance for classifying whether a patient has cardiovascular disease, the improvement does not significantly alter the general performance of the models. For example, with a ROC value at 0.777 and 0.768 for the k-NN model and the logistic regression model still include a lot of error that may cause harm to a person who is predicted as having cardiovascular disease but does not. Therefore, while these models show improved performance in classifying a patient having cardiovascular disease, the k-NN model and the logistic regression model may not provide valuable predictions based on the ROC values. Stated differently, these improved models having optimized parameters still have ROC values that include predicting a large number of people as having cardiovascular disease when they actually do not. As above described, this misclassification provides potential for those people to incur harm, and thus, would likely need even further improvement before implementation and deployment.

In another instance, the attributes were split into numerical and categorical features, then the pre-processing was conducted once more through the pipelines. After the data returns from the pipelines, the smoking dataset was passed through to the “RandomizedSearchCV()” function to find the optimal performance of each of the models. It is important to remember that the performance of these algorithms are evaluated against the recall metric to understand the false-negative rate of each of the algorithms. After running the “RandomizedSearchCV()” function, each of the classification models calculated a recall value. Shown in the below cell, the recall for the random forest algorithm is 0.673, the recall for the decision tree algorithm is 0.830, and the recall for the gradient boosting algorithm is 0.868. Therefore, in comparison, the gradient boosting model still provides the greatest recall value between the competing models and provides the greatest indication of accurately predicting whether a person is a smoker. Moving on to the next best performing model, there's the decision tree model. The recall value for the decision tree model improved significantly using the “RandomizedSearchCV()” function, while the performance of the random forest model also increased slightly. The recall from the decision tree model is now only slightly less than the recall for the gradient boosting model, while the random forest model is still significantly less than the decision tree model and the gradient boosting model. This suggests that, while the recall values suggest that the gradient boosting model is the most appropriate, the decision tree model is competitive. In order to predict whether someone is a smoker or not, each of these recall values may have secondary effects, which may impact deployment, such as computation time. Since the harm to a person to be wrongly classified as a smoker is one of inconvenience, discussed previously, a recall value as predicted may provide for a deployable model.

The decision tree and gradient boosting models all have recall values greater than 0.80. This high recall for each of the competing models suggest that few false negatives are present in the prediction. This provides valuable predictions to identify a person who may require higher health care costs by being a smoker, and thus subjected to higher health insurance premiums. For example, with a recall value at 0.868 and 0.830, the gradient boosting and decision tree models, respectively, perform well in accurately predicting a person as a smoker with minimizing the predictions of non-smokers as smokers. These recall values may allow for better identification of smokers who may have avoided paying higher premiums due to their smoking. In the instance of classifying a smoker in the smoking dataset, the “RandomizedSearchCV()” function provided improved results that may allow for potential deployment of these models.

The results of the classification models for both the cardiovascular disease dataset and the smoking dataset are better evaluated using visualizations.


Check prediction against entire set using SMOTE, and just the Tree classifier (we could have used any, this is just an example)
The purpose of the next two parts is to show how Recall is affected if SMOTE is applied to the entire set.

Now do same test, but against the imbalanced set (No SMOTE). The accuracy of this set is higher, but has a much lower recall score compared to SMOTE above.

Modeling and Evaluation 4

Performance for each of the models may be compared using visualizations. Shown in the below cell is a 4x4 grid depicting each ROC curve for each of the models. Specifically, the ROC curves each depict the ROC curve of the class “Healthy” and the ROC curve of the class “CVD.” The below graphs also depict micro and macro average ROC curves for each of the classification models used for each of the tasks. The visualization is provided using the “yellowbrick” Python package. Specifically, the “ROCAUC” library is imported from the “classifier” component of the “yellowbrick” Python package. The “ROCAUC” library allows for efficient generation of the ROC curves shown by providing easy inputs through a for loop to provide a visualization for each of the models in each of the tasks. For each task, that is classifying cardiovascular disease and classifying if a person is a smoker, the ROC graphs shown are generated using parameters from the “GridSearchCV()” function from the “scikit-learn” Python package. Generally, the “GridSearchCV()” function provides output with optimal parameters by comparing all of the parameters and finding the parameter that produces the most optimal value. (https://scikit-learn.org/stable/modules/grid_search.html#grid-search).

For the cardiovascular disease dataset, the graphs provide visualizations depicting each ROC value using a traditional ROC graph. As described above, the ROC graph includes a comparison to a straight, diagonal line being indicative of random guessing. Specifically, the ROC values from the k-NN model, the Naïve-Bayes model, and the logistic regression model are depicted graphically against the random guess diagonal line. As expected, the ROC curve shown for the k-NN model shows a sharp increase closer to the origin, which is consistent with a small false positive rate. Since the k-NN model has the highest ROC value, a small false positive rate is expected. Similarly, the ROC curve for each of the logistic regression and Naïve-Bayes models follow a similar analysis. As shown the ROC curve for logistic regression model is like the k-NN in having a sharp increase near the origin, while the Naïve-Bayes ROC curve appears flatter against the random guess line. This is indicative of the Naïve-Bayes model having less area under the ROC curve than the k-NN and logistic regression models. This visualization is interesting to someone who might use this model because the visualization depicts the area under the ROC curve for each of the k-NN, logistic regression, and Naïve-Bayes models, which makes the ROC value more readily understandable.

For the smoking dataset, the graphs provide visualizations depicting each recall value against the precision value for each of the models. These visualizations provide a visual indication of the number of false positive rate included in the prediction compared the true positive rate. Specifically, the recall values for the random forest, decision tree, and gradient boosting models are depicted against the precision values for each of the models. Shown on each graph is the recall versus the precision for the random forest, gradient boosting, and decision tree models, respectively. Generally, with all of the models, as the recall increases, the precision decreases. The precision-recall curve for the random forest model includes a substantially linear curve having a negative slope indicating an inverse relationship between precision and recall. The precision-recall curve for the gradient boosting model shows a stepped relationship between precision and recall. Lastly, the decision tree model shows characteristics of both having a linear curve while also having stepped portions. These differences can be attributed to the differences used to calculate the recall values in each of the algorithms. This visualization is interesting to someone who might use the model because the visualizations shows how the recall value was calculated, for example stepped, or linear, as well providing a visual indication of how the optimal recall compares to the average precision.

Also, for the smoking dataset, a classification report is generated to provide a visual indication of the various performance metrics for each of the classification models. As shown, each classification report for the random forest model, the gradient boosting model, and the decision tree model includes a heatmap showing values closer to one in darker color, in this case red, from the other colors. The metrics depicted for each of the models includes the precision, the recall, and the F1 score. The classification report depicts values for each of the models generated using the “GridSearchCV()” function. The classification report generates a visual that provides information in various ways such as through a heatmap as well as numerically to convey the performance metrics for each of the random forest, gradient boosting, and decision tree classification models. This visualization is important to someone who might use this model because the visualization depicts the strength of the recall and other performance metrics using color as well as numerical values making the model performance more readily understandable.

Modeling and Evaluation 5

Below we were asked to compare the various models for each classification task. Our two tasks outlined in this project were to discover the presence or classify Cardiovascular Disease as well as to classify whether a given observation or individual was a smoker. These tasks were completed independently of one another.Stated differently, the tests were not run in order to find whether CDV was always present with a smoker, or the likelihood of a smoker having CVD and their counter-components. The tasks set were to discover simply if CVD would be present given a set of variables or attributes and if an individual was a smoker or non-smoker given a set of variables or attributes.

Within each task, classifying CVD and classifying smoker and non-smoker, various modelling methods were utilized. The performance of each model was then compared to another providing us a t-statistic and a p-value. The t-value provides a ratio of departure from an estimated value which is hypothesized via the standard error. This then can be used in a t-test to calculate a t-statistic which aids in the acceptance or rejection of the null hypothesis. Essentially, we are able to derive the basis of a confidence interval from this value and thus say with an amount of certainty that we are able to accept or reject the null-hypothesis, in this case that models are performing equally or not equally. That is to say, the further the way from zero that the t-statistic is, the further away from the mean we can expect to be. Thus, our confidence in that test would be lower as our interval would be wider, regardless of the direction in which it deviates from zero.

Additionally, another commonly used metric was included. The p-value is provided with each comparison of model performances. With this particular metric, the smaller the number the stronger the evidence is that the null hypothesis should be rejected. To restate, the null hypothesis here is that the models are performing equally, meaning that as their metrics are closer to one another, we should see the p-value decrease or become smaller. If the metric becomes larger, there is a greater deviation from one another, suggesting that we should reject the null hypothesis. The other component to the p-value is that we set an interval at which to test. This report has been developed to evaluate each model against itself (within each respective task) by using 95% confidence. That is to say that if a p-value is less than.0005 and a t-statistic relatively close to 0, we can confidently reject the null hypothesis. Which stated again, is that the models are performing equally well.

In the task of classifying CVD, Naïve Bayes was compared to KNN and the null hypothesis was rejected, presuming that Naïve Bayes performed better than K-Nearest Neighbors. In the second comparison, we evaluated Naïve Bayes against Logistic Regression, where although we had a low p-value we found a t-statistic to be much different than zero, again rejecting the null hypothesis that the models perform equally well. Finally, K-Nearest Neighbors was compared to Logistic Regression, where again a low p-value was discovered but a t-statistic provided us additional evidence to reject the null hypothesis of the models performing equally well.

In our second task, classifying an individual as a smoker or non-smoker, we compared our Random Forest model to our Decision Tree model. In this comparison, we were unable to reject the null hypothesis concluding that the two algorithms are not significantly different. Then the Random Forest was compared to the Gradient Boosting model, where the null hypothesis was rejected as the models do not perform equally as well. Lastly, the Decision Tree model was compared to the Gradient Boosting model, where the null hypothesis was again rejected as the models do not perform equally as well either.

Reject the null hypothesis that the models perform equally well.

Reject the null hypothesis that the models perform equally well.

Reject the null hypothesis that the models perform equally well.

Fail to Reject, and we may conclude that the performance of the two algorithms is not significantly different.

Reject the null hypothesis that the models perform equally well.

Reject the null hypothesis that the models perform equally well.

Fail to Reject, and we may conclude that the performance of the two algorithms is not significantly different.

Modeling and Evaluation 6

Using the parameters found from the GridSearch() function, a logistic regression model is executed in the below cell.

The above graph depicts feature weights for each of the features in the logistic regression model. Based on the above graph, the features that contribute significantly to this model include “Age,” “Cholesterol,” “Blood Pressure” and the” BMI Group”. Other attributes, like “Gender,” “Smoking,” “Alcohol” and “Activity” don't appear to as strong of indicators for the presence of cardiovascular disease. For example, each of the features “Age,” “Cholesterol, “Blood Pressure” and the” BMI Group” have strong weights that positively extend to around 0.5. Likewise, some of the remaining features have some positive extending weights, such as “Smoking,” “Alcohol” and “Activity.” These weights, however, only appear to extend to around 0.1.

Using common domain knowledge, the features “Age,” “Cholesterol,” “Blood Pressure” and the” BMI Group” appear to be consistent with attributes that would contribute significantly to predicting the presence of cardiovascular disease. For example, as a person ages, so does the heart tissue, which would likely result in more cardiovascular injuries. Similarly, a high cholesterol value is indicative of a high presence of a substance commonly known to cause artery blockage. Lastly, a similar analysis applies to the “Blood Pressure” and the” BMI Group” features. As the value in each of those features increases, the likelihood of cardiovascular injury also increases, for example through increased arterial pressure or a high body mass. The other positively extending features also depict that some behaviors, such as smoking and alcohol intake, and inactivity may contribute slightly to the presence of cardiovascular disease. This also matches the common domain knowledge of cardiovascular disease causes. The below cell executes a further logistic regression model using the 4, strongest, weighted features to analyze changes in model performance.

Feature Importance
The feature importance on gender was insteresting, but we saw previously that almost all of the smokers in this dataset were males, which may be skewing the data. Unfortunately, we ran out of time validating this. But a quick glance at the data in Excel and using pivot tables, you can see that for one gender, almost all of them are flagged as a smoker.

Deployment
For the cardiovascular disease dataset, the models analyzed do not achieve ROC AUC values greater than 0.783 using gradient boosting as above described. For the smoking dataset, the models analyzed do not achieve recall values greater than 0.71. This suggests that the models would not be very useful for interested parties.

In the cardiovascular disease dataset, the ROC AUC values indicate that there is a relatively high amount of false-positive results. Specifically, approximately 30% of the data is indicative of a false positive result from using this model. This equates to treating cardiovascular disease in which 30% of the patients likely do not have cardiovascular disease. With such a large percentage and harmful results, it is unlikely that the model used to predict cardiovascular disease would not be adopted or deployed. If the model for the cardiovascular disease dataset were to be deployed, the value of the model would be measured by the number of patients that do not have cardiovascular disease but were treated for cardiovascular disease. The model used for the cardiovascular disease dataset would be deployed inside a hospital setting, and accessible at a nurse’s station. This would allow a nurse to input the relevant data, run the model, and provide results to a doctor for analysis.

Other data that may be collected to improve the performance of the model may include such measurements as commonly used for diagnosing cardiovascular disease. These may include lipids, proteins or other, known causes of cardiovascular disease. (https://www.ahajournals.org/doi/full/10.1161/01.CIR.0000114134.03187.7B). The model would need to be updated every time a person suffering cardiovascular disease is admitted or visits the hospital. This would provide more data for the model to evaluate and predict and may improve usefulness.

In the smoking dataset, the recall values also indicate that a substantial percentage, approximately 30%, of the predictions include false negative predictions. This would equate to approximately 30% of customers being labeled as not smokers who are smokers, and thus incurring liability for higher costs.

Misclassiofiying 30$ of people needing higher cost medical care may have devasating impacts on a business model. Therefore, the model used to predict a smoker is unlikely to be adopted or deployed by health insurance companies. If the model were to be deployed, the value of the model would be measured by the number of customers who change insurers based on high premium costs that result because of the higher-cost individuals. This would indicate non-smokers being charged more, and, at least those customers are more likely to get a better, competing rate. The model for the smoking dataset would be deployed at a health insurance company and applied when a new application is received by the company.

Additional data that could support this notebook may exist as the presence of health problems commonly associated with lung disease that are known to be caused by smoking. These may include the presence of tar on lungs, or a diagnosis of COPD or other respiratory illness caused by smoking. (https://www.cdc.gov/tobacco/data_statistics/fact_sheets/health_effects/effects_cig_smoking/index.htm#:~:text=Smoking%20can%20cause%20lung%20disease,alveoli)%20found%20in%20your%20lungs.&text=Lung%20diseases%20caused%20by%20smoking,includes%20emphysema%20and%20chronic%20bronchitis.&text=Cigarette%20smoking%20causes%20most%20cases%20of%20lung%20cancer.)

Whenever new applications were accepted, they would have be run against the processes in this notebook each time an individual applied for health insurance from that given company. In every instance which someone provided their data for comparison to the results here, it would continue to improve the classification models outlined and predictions could continue to provide additional usefulness.

Exceptional Work

Important - We realise this seciton will not be graded with resubmission but we rely on the code below for our notebook, so we will keep it in.

As discussed above, our exceptional work outlines the "RandomizedSearchCV()" and "GridSearchCV()" functions. More information can be found in earlier sections.

Allow the link below to also serve as an additional reference for these functions: (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html).

For the cardiovascular disease dataset, the most significant parameter for the k-NN model is the number of nearest neighbors, which in this instance is 49. The most significant parameter for the Naïve-Bayes model is the alpha value, which in this instance is 1. The most significant parameter for the logistic regression model is penalty, which in this instance is 12.

For the smoking dataset, the most significant parameter for the random forest model is. The most significant parameter for the decision tree model is the minimum samples per leaf, which in this instance is 4. The most significant parameter for the gradient boosting model is the number of estimators, which in this instance is 4.



