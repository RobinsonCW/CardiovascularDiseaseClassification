{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595451868010",
   "display_name": "Python 3.7.1 64-bit ('ML7331': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Chance\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "t boosting model still provides the greatest recall value between the competing models and provides the greatest indication of accurately predicting whether a person is a smoker. Moving on to the next best performing model, there\\'s the decision tree model. The recall value for the decision tree model improved significantly using the “RandomizedSearchCV()” function, while the performance of the random forest model also increased slightly. The recall from the decision tree model is now only slightly less than the recall for the gradient boosting model, while the random forest model is still significantly less than the decision tree model and the gradient boosting model. This suggests that, while the recall values suggest that the gradient boosting model is the most appropriate, the decision tree model is competitive. In order to predict whether someone is a smoker or not, each of these recall values may have secondary effects, which may impact deployment, such as computation time. Since the harm to a person to be wrongly classified as a smoker is one of inconvenience, discussed previously, a recall value as predicted may provide for a deployable model.\\n\\nThe decision tree and gradient boosting models all have recall values greater than 0.80. This high recall for each of the competing models suggest that few false negatives are present in the prediction. This provides valuable predictions to identify a person who may require higher health care costs by being a smoker, and thus subjected to higher health insurance premiums. For example, with a recall value at 0.868 and 0.830, the gradient boosting and decision tree models, respectively, perform well in accurately predicting a person as a smoker with minimizing the predictions of non-smokers as smokers. These recall values may allow for better identification of smokers who may have avoided paying higher premiums due to their smoking. In the instance of classifying a smoker in the smoking dataset, the “RandomizedSearchCV()” function provided improved results that may allow for potential deployment of these models.\\n\\nThe results of the classification models for both the cardiovascular disease dataset and the smoking dataset are better evaluated using visualizations.\\n\\n\\nCheck prediction against entire set using SMOTE, and just the Tree classifier (we could have used any, this is just an example)\\nThe purpose of the next two parts is to show how Recall is affected if SMOTE is applied to the entire set.\\n\\nNow do same test, but against the imbalanced set (No SMOTE). The accuracy of this set is higher, but has a much lower recall score compared to SMOTE above.\\n\\nModeling and Evaluation 4\\n\\nPerformance for each of the models may be compared using visualizations. Shown in the below cell is a 4x4 grid depicting each ROC curve for each of the models. Specifically, the ROC curves each depict the ROC curve of the class “Healthy” and the ROC curve of the class “CVD.” The below graphs also depict micro and macro average ROC curves for each of the classification models used for each of the tasks. The visualization is provided using the “yellowbrick” Python package. Specifically, the “ROCAUC” library is imported from the “classifier” component of the “yellowbrick” Python package. The “ROCAUC” library allows for efficient generation of the ROC curves shown by providing easy inputs through a for loop to provide a visualization for each of the models in each of the tasks. For each task, that is classifying cardiovascular disease and classifying if a person is a smoker, the ROC graphs shown are generated using parameters from the “GridSearchCV()” function from the “scikit-learn” Python package. Generally, the “GridSearchCV()” function provides output with optimal parameters by comparing all of the parameters and finding the parameter that produces the most optimal value. (https://scikit-learn.org/stable/modules/grid_search.html#grid-search).\\n\\nFor the cardiovascular disease dataset, the graphs provide visualizations depicting each ROC value using a traditional ROC graph. As described above, the ROC graph includes a comparison to a straight, diagonal line being indicative of random guessing. Specifically, the ROC values from the k-NN model, the Naïve-Bayes model, and the logistic regression model are depicted graphically against the random guess diagonal line. As expected, the ROC curve shown for the k-NN model shows a sharp increase closer to the origin, which is consistent with a small false positive rate. Since the k-NN model has the highest ROC value, a small false positive rate is expected. Similarly, the ROC curve for each of the logistic regression and Naïve-Bayes models follow a similar analysis. As shown the ROC curve for logistic regression model is like the k-NN in having a sharp increase near the origin, while the Naïve-Bayes ROC curve appears flatter against the random guess line. This is indicative of the Naïve-Bayes model having less area under the ROC curve than the k-NN and logistic regression models. This visualization is interesting to someone who might use this model because the visualization depicts the area under the ROC curve for each of the k-NN, logistic regression, and Naïve-Bayes models, which makes the ROC value more readily understandable.\\n\\nFor the smoking dataset, the graphs provide visualizations depicting each recall value against the precision value for each of the models. These visualizations provide a visual indication of the number of false positive rate included in the prediction compared the true positive rate. Specifically, the recall values for the random forest, decision tree, and gradient boosting models are depicted against the precision values for each of the models. Shown on each graph is the recall versus the precision for the random forest, gradient boosting, and decision tree models, respectively. Generally, with all of the models, as the recall increases, the precision decreases. The precision-recall curve for the random forest model includes a substantially linear curve having a negative slope indicating an inverse relationship between precision and recall. The precision-recall curve for the gradient boosting model shows a stepped relationship between precision and recall. Lastly, the decision tree model shows characteristics of both having a linear curve while also having stepped portions. These differences can be attributed to the differences used to calculate the recall values in each of the algorithms. This visualization is interesting to someone who might use the model because the visualizations shows how the recall value was calculated, for example stepped, or linear, as well providing a visual indication of how the optimal recall compares to the average precision.\\n\\nAlso, for the smoking dataset, a classification report is generated to provide a visual indication of the various performance metrics for each of the classification models. As shown, each classification report for the random forest model, the gradient boosting model, and the decision tree model includes a heatmap showing values closer to one in darker color, in this case red, from the other colors. The metrics depicted for each of the models includes the precision, the recall, and the F1 score. The classification report depicts values for each of the models generated using the “GridSearchCV()” function. The classification report generates a visual that provides information in various ways such as through a heatmap as well as numerically to convey the performance metrics for each of the random forest, gradient boosting, and decision tree classification models. This visualization is important to someone who might use this model because the visualization depicts the strength of the recall and other performance metrics using color as well as numerical values making the model performance more readily understandable.\\n\\nModeling and Evaluation 5\\n\\nBelow we were asked to compare the various models for each classification task. Our two tasks outlined in this project were to discover the presence or classify Cardiovascular Disease as well as to classify whether a given observation or individual was a smoker. These tasks were completed independently of one another.Stated differently, the tests were not run in order to find whether CDV was always present with a smoker, or the likelihood of a smoker having CVD and their counter-components. The tasks set were to discover simply if CVD would be present given a set of variables or attributes and if an individual was a smoker or non-smoker given a set of variables or attributes.\\n\\nWithin each task, classifying CVD and classifying smoker and non-smoker, various modelling methods were utilized. The performance of each model was then compared to another providing us a t-statistic and a p-value. The t-value provides a ratio of departure from an estimated value which is hypothesized via the standard error. This then can be used in a t-test to calculate a t-statistic which aids in the acceptance or rejection of the null hypothesis. Essentially, we are able to derive the basis of a confidence interval from this value and thus say with an amount of certainty that we are able to accept or reject the null-hypothesis, in this case that models are performing equally or not equally. That is to say, the further the way from zero that the t-statistic is, the further away from the mean we can expect to be. Thus, our confidence in that test would be lower as our interval would be wider, regardless of the direction in which it deviates from zero.\\n\\nAdditionally, another commonly used metric was included. The p-value is provided with each comparison of model performances. With this particular metric, the smaller the number the stronger the evidence is that the null hypothesis should be rejected. To restate, the null hypothesis here is that the models are performing equally, meaning that as their metrics are closer to one another, we should see the p-value decrease or become smaller. If the metric becomes larger, there is a greater deviation from one another, suggesting that we should reject the null hypothesis. The other component to the p-value is that we set an interval at which to test. This report has been developed to evaluate each model against itself (within each respective task) by using 95% confidence. That is to say that if a p-value is less than.0005 and a t-statistic relatively close to 0, we can confidently reject the null hypothesis. Which stated again, is that the models are performing equally well.\\n\\nIn the task of classifying CVD, Naïve Bayes was compared to KNN and the null hypothesis was rejected, presuming that Naïve Bayes performed better than K-Nearest Neighbors. In the second comparison, we evaluated Naïve Bayes against Logistic Regression, where although we had a low p-value we found a t-statistic to be much different than zero, again rejecting the null hypothesis that the models perform equally well. Finally, K-Nearest Neighbors was compared to Logistic Regression, where again a low p-value was discovered but a t-statistic provided us additional evidence to reject the null hypothesis of the models performing equally well.\\n\\nIn our second task, classifying an individual as a smoker or non-smoker, we compared our Random Forest model to our Decision Tree model. In this comparison, we were unable to reject the null hypothesis concluding that the two algorithms are not significantly different. Then the Random Forest was compared to the Gradient Boosting model, where the null hypothesis was rejected as the models do not perform equally as well. Lastly, the Decision Tree model was compared to the Gradient Boosting model, where the null hypothesis was again rejected as the models do not perform equally as well either.\\n\\nReject the null hypothesis that the models perform equally well.\\n\\nReject the null hypothesis that the models perform equally well.\\n\\nReject the null hypothesis that the models perform equally well.\\n\\nFail to Reject, and we may conclude that the performance of the two algorithms is not significantly different.\\n\\nReject the null hypothesis that the models perform equally well.\\n\\nReject the null hypothesis that the models perform equally well.\\n\\nFail to Reject, and we may conclude that the performance of the two algorithms is not significantly different.\\n\\nModeling and Evaluation 6\\n\\nUsing the parameters found from the GridSearch() function, a logistic regression model is executed in the below cell.\\n\\nThe above graph depicts feature weights for each of the features in the logistic regression model. Based on the above graph, the features that contribute significantly to this model include “Age,” “Cholesterol,” “Blood Pressure” and the” BMI Group”. Other attributes, like “Gender,” “Smoking,” “Alcohol” and “Activity” don\\'t appear to as strong of indicators for the presence of cardiovascular disease. For example, each of the features “Age,” “Cholesterol, “Blood Pressure” and the” BMI Group” have strong weights that positively extend to around 0.5. Likewise, some of the remaining features have some positive extending weights, such as “Smoking,” “Alcohol” and “Activity.” These weights, however, only appear to extend to around 0.1.\\n\\nUsing common domain knowledge, the features “Age,” “Cholesterol,” “Blood Pressure” and the” BMI Group” appear to be consistent with attributes that would contribute significantly to predicting the presence of cardiovascular disease. For example, as a person ages, so does the heart tissue, which would likely result in more cardiovascular injuries. Similarly, a high cholesterol value is indicative of a high presence of a substance commonly known to cause artery blockage. Lastly, a similar analysis applies to the “Blood Pressure” and the” BMI Group” features. As the value in each of those features increases, the likelihood of cardiovascular injury also increases, for example through increased arterial pressure or a high body mass. The other positively extending features also depict that some behaviors, such as smoking and alcohol intake, and inactivity may contribute slightly to the presence of cardiovascular disease. This also matches the common domain knowledge of cardiovascular disease causes. The below cell executes a further logistic regression model using the 4, strongest, weighted features to analyze changes in model performance.\\n\\nFeature Importance\\nThe feature importance on gender was insteresting, but we saw previously that almost all of the smokers in this dataset were males, which may be skewing the data. Unfortunately, we ran out of time validating this. But a quick glance at the data in Excel and using pivot tables, you can see that for one gender, almost all of them are flagged as a smoker.\\n\\nDeployment\\nFor the cardiovascular disease dataset, the models analyzed do not achieve ROC AUC values greater than 0.783 using gradient boosting as above described. For the smoking dataset, the models analyzed do not achieve recall values greater than 0.71. This suggests that the models would not be very useful for interested parties.\\n\\nIn the cardiovascular disease dataset, the ROC AUC values indicate that there is a relatively high amount of false-positive results. Specifically, approximately 30% of the data is indicative of a false positive result from using this model. This equates to treating cardiovascular disease in which 30% of the patients likely do not have cardiovascular disease. With such a large percentage and harmful results, it is unlikely that the model used to predict cardiovascular disease would not be adopted or deployed. If the model for the cardiovascular disease dataset were to be deployed, the value of the model would be measured by the number of patients that do not have cardiovascular disease but were treated for cardiovascular disease. The model used for the cardiovascular disease dataset would be deployed inside a hospital setting, and accessible at a nurse’s station. This would allow a nurse to input the relevant data, run the model, and provide results to a doctor for analysis.\\n\\nOther data that may be collected to improve the performance of the model may include such measurements as commonly used for diagnosing cardiovascular disease. These may include lipids, proteins or other, known causes of cardiovascular disease. (https://www.ahajournals.org/doi/full/10.1161/01.CIR.0000114134.03187.7B). The model would need to be updated every time a person suffering cardiovascular disease is admitted or visits the hospital. This would provide more data for the model to evaluate and predict and may improve usefulness.\\n\\nIn the smoking dataset, the recall values also indicate that a substantial percentage, approximately 30%, of the predictions include false negative predictions. This would equate to approximately 30% of customers being labeled as not smokers who are smokers, and thus incurring liability for higher costs.\\n\\nMisclassiofiying 30$ of people needing higher cost medical care may have devasating impacts on a business model. Therefore, the model used to predict a smoker is unlikely to be adopted or deployed by health insurance companies. If the model were to be deployed, the value of the model would be measured by the number of customers who change insurers based on high premium costs that result because of the higher-cost individuals. This would indicate non-smokers being charged more, and, at least those customers are more likely to get a better, competing rate. The model for the smoking dataset would be deployed at a health insurance company and applied when a new application is received by the company.\\n\\nAdditional data that could support this notebook may exist as the presence of health problems commonly associated with lung disease that are known to be caused by smoking. These may include the presence of tar on lungs, or a diagnosis of COPD or other respiratory illness caused by smoking. (https://www.cdc.gov/tobacco/data_statistics/fact_sheets/health_effects/effects_cig_smoking/index.htm#:~:text=Smoking%20can%20cause%20lung%20disease,alveoli)%20found%20in%20your%20lungs.&text=Lung%20diseases%20caused%20by%20smoking,includes%20emphysema%20and%20chronic%20bronchitis.&text=Cigarette%20smoking%20causes%20most%20cases%20of%20lung%20cancer.)\\n\\nWhenever new applications were accepted, they would have be run against the processes in this notebook each time an individual applied for health insurance from that given company. In every instance which someone provided their data for comparison to the results here, it would continue to improve the classification models outlined and predictions could continue to provide additional usefulness.\\n\\nExceptional Work\\n\\nImportant - We realise this seciton will not be graded with resubmission but we rely on the code below for our notebook, so we will keep it in.\\n\\nAs discussed above, our exceptional work outlines the \"RandomizedSearchCV()\" and \"GridSearchCV()\" functions. More information can be found in earlier sections.\\n\\nAllow the link below to also serve as an additional reference for these functions: (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html).\\n\\nFor the cardiovascular disease dataset, the most significant parameter for the k-NN model is the number of nearest neighbors, which in this instance is 49. The most significant parameter for the Naïve-Bayes model is the alpha value, which in this instance is 1. The most significant parameter for the logistic regression model is penalty, which in this instance is 12.\\n\\nFor the smoking dataset, the most significant parameter for the random forest model is. The most significant parameter for the decision tree model is the minimum samples per leaf, which in this instance is 4. The most significant parameter for the gradient boosting model is the number of estimators, which in this instance is 4.\\n\\n\\n\\n'"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "with open (\"./Lab_2_TEXT.txt\", \"r\", encoding=\"utf-8\") as myfile:\n",
    "    data=myfile.read()  \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(5, 'Reject the null hypothesis that the models perform equally well.'),\n (2,\n  'Fail to Reject, and we may conclude that the performance of the two algorithms is not significantly different.')]"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(data)\n",
    "\n",
    "duplicates = list(set([s for s in sentences if sentences.count(s) > 1]))\n",
    "cleaned = list(set(sentences))\n",
    "#duplicates\n",
    "\n",
    "dupCtsBySentence = list(set([(sentences.count(s), s) for s in sentences if sentences.count(s) > 1]))\n",
    "dupCtsBySentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Duplicated Sentences: 7\n"
    }
   ],
   "source": [
    "# duplicatedSentences = [sentences.count(s) for s in sentences if sentences.count(s) > 1]\n",
    "duplicatedSentencesCt = sum([i[0] for i in dupCtsBySentence])\n",
    "print('Duplicated Sentences:', duplicatedSentencesCt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total Sentences: 367\n"
    }
   ],
   "source": [
    "totalSentences = len(sentences)\n",
    "print('Total Sentences:', totalSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Duplicate Percentage: 1.91\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Duplicate Percentage:', np.round((duplicatedSentencesCt / totalSentences)*100,2))"
   ]
  }
 ]
}